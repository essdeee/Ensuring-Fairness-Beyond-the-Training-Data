{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we find the baseline fairness using each of the three fairness algorithms we would like to compare: Reweighing (Preprocessing), Reductions Approach to Fair Classficiation (Inprocessing), and Equality of Opportunity in Supervised Learning (Postprocessing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def evaluate_fairness(y_true, y_pred, sensitive_features):\n",
    "    \"\"\"\n",
    "    Evaluates fairness of the final majority vote classifier over T_inner hypotheses\n",
    "    on the test set.\n",
    "    #NOTE: defined in the meta_algo file, but we chose:\n",
    "    a0 := African-American (COMPAS), Female (Adult)\n",
    "    a1 := Caucasian (COMPAS), Male (Adult)\n",
    "\n",
    "    :return: list. subgroups in sensitive_features.\n",
    "    :return: list, dict, dict. groups is a list of the sensitive features in the dataset. \n",
    "    group_metrics is a dictionary containing dictionaries that have Delta_dp, Delta_eoy0, \n",
    "    and Delta_eoy1 for each group. gaps is a dictionary that contains the fairness gap\n",
    "    for dp, eo_y0 and eo_y1.\n",
    "    \"\"\"\n",
    "    groups = np.unique(sensitive_features.values)\n",
    "    pos_count = {}\n",
    "    dp_pct = {}\n",
    "    eo_y0_pct = {}\n",
    "    eo_y1_pct = {}\n",
    "\n",
    "    for index, group in enumerate(groups):\n",
    "        # Demographic Parity\n",
    "        indices = {}\n",
    "        indices[group] = sensitive_features.index[sensitive_features == group]\n",
    "        dp_pct[group] = sum(y_pred[indices[group]])/len(indices[group])\n",
    "\n",
    "        # Equalized Odds\n",
    "        y1_indices = {}\n",
    "        y0_indices = {}\n",
    "        y1_indices[group] = sensitive_features.index[(sensitive_features == group) & (y_true == 1)]\n",
    "        y0_indices[group] = sensitive_features.index[(sensitive_features == group) & (y_true == 0)]\n",
    "        eo_y0_pct[group] = sum(y_pred[y0_indices[group]])/len(y0_indices[group])   \n",
    "        eo_y1_pct[group] = sum(y_pred[y1_indices[group]])/len(y1_indices[group])\n",
    "\n",
    "    gaps = {}\n",
    "    group_metrics = {} # a dictionary of dictionaries\n",
    "\n",
    "    gaps['dp'] = abs(dp_pct[groups[0]] - dp_pct[groups[1]])\n",
    "    gaps['eo_y0'] = abs(eo_y0_pct[groups[0]] - eo_y0_pct[groups[1]])\n",
    "    gaps['eo_y1'] = abs(eo_y1_pct[groups[0]] - eo_y1_pct[groups[1]])\n",
    "    group_metrics['dp'] = dp_pct\n",
    "    group_metrics['eo_y0'] = eo_y0_pct\n",
    "    group_metrics['eo_y1'] = eo_y1_pct\n",
    "\n",
    "    return groups, group_metrics, gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('./../../data/processed/compas/compas_train1_X.csv')\n",
    "X_test = pd.read_csv('./../../data/processed/compas/compas_test1_X.csv')\n",
    "y_train = pd.read_csv('./../../data/processed/compas/compas_train1_y.csv')\n",
    "y_train = y_train['two_year_recid']\n",
    "y_test = pd.read_csv('./../../data/processed/compas/compas_test1_y.csv')\n",
    "y_test = y_test['two_year_recid']\n",
    "\n",
    "sensitive_features_train = X_train['race']\n",
    "sensitive_features_test = X_test['race']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPAS Test Accuracy (Unfair): 0.6926315789473684\n"
     ]
    }
   ],
   "source": [
    "print(\"COMPAS Test Accuracy (Unfair): {}\".format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups, group_metrics, gaps = evaluate_fairness(y_test, y_pred, sensitive_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[h(X) = 1 | A = 0.0] = 0.4963768115942029\n",
      "P[h(X) = 1 | A = 1.0] = 0.24623115577889448\n",
      "Delta_dp = 0.2501456558153084\n",
      "P[h(X) = 1 | A = 0.0, Y = 1] = 0.6948529411764706\n",
      "P[h(X) = 1 | A = 0.0, Y = 0] = 0.30357142857142855\n",
      "P[h(X) = 1 | A = 1.0, Y = 1] = 0.41216216216216217\n",
      "P[h(X) = 1 | A = 1.0, Y = 0] = 0.148\n",
      "Delta_eo1 = 0.2826907790143084\n",
      "Delta_eo0 = 0.15557142857142855\n"
     ]
    }
   ],
   "source": [
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}] = {}\".format(group, group_metrics['dp'][group]))\n",
    "print(\"Delta_dp = {}\".format(gaps['dp']))\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 1] = {}\".format(group, group_metrics['eo_y1'][group]))\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 0] = {}\".format(group, group_metrics['eo_y0'][group]))\n",
    "print(\"Delta_eo1 = {}\".format(gaps['eo_y1']))\n",
    "print(\"Delta_eo0 = {}\".format(gaps['eo_y0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity, EqualizedOdds\n",
    "\n",
    "expgrad_dp = ExponentiatedGradient(\n",
    "    LogisticRegression(solver='liblinear', fit_intercept=True, class_weight='balanced'),\n",
    "    constraints=DemographicParity(),\n",
    "    eps=0.05,\n",
    "    nu=1e-6)\n",
    "\n",
    "expgrad_eo = ExponentiatedGradient(\n",
    "    LogisticRegression(solver='liblinear', fit_intercept=True, class_weight='balanced'),\n",
    "    constraints=EqualizedOdds(),\n",
    "    eps=0.05,\n",
    "    nu=1e-6)\n",
    "\n",
    "expgrad_dp.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "expgrad_eo.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "y_pred_dp = expgrad_dp.predict(X_test)\n",
    "y_pred_eo = expgrad_eo.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPAS Test Accuracy (Inprocessing, DP): 0.6789473684210526\n",
      "COMPAS Test Accuracy (Inprocessing, EO): 0.6747368421052632\n"
     ]
    }
   ],
   "source": [
    "print(\"COMPAS Test Accuracy (Inprocessing, DP): {}\".format(accuracy_score(y_pred_dp, y_test)))\n",
    "print(\"COMPAS Test Accuracy (Inprocessing, EO): {}\".format(accuracy_score(y_pred_eo, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups, group_metrics_dp, gaps_dp = evaluate_fairness(y_test, y_pred_dp, sensitive_features_test)\n",
    "groups, group_metrics_eo, gaps_eo = evaluate_fairness(y_test, y_pred_eo, sensitive_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[h(X) = 1 | A = 0.0] = 0.5018115942028986\n",
      "P[h(X) = 1 | A = 1.0] = 0.4020100502512563\n",
      "Delta_dp = 0.09980154395164226\n",
      "P[h(X) = 1 | A = 0.0, Y = 1] = 0.7022058823529411\n",
      "P[h(X) = 1 | A = 0.0, Y = 0] = 0.3142857142857143\n",
      "P[h(X) = 1 | A = 1.0, Y = 1] = 0.5675675675675675\n",
      "P[h(X) = 1 | A = 1.0, Y = 0] = 0.304\n",
      "Delta_eo1 = 0.13463831478537358\n",
      "Delta_eo0 = 0.010285714285714287\n"
     ]
    }
   ],
   "source": [
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}] = {}\".format(group, group_metrics_dp['dp'][group]))\n",
    "print(\"Delta_dp = {}\".format(gaps_dp['dp']))\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 1] = {}\".format(group, group_metrics_eo['eo_y1'][group]))\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 0] = {}\".format(group, group_metrics_eo['eo_y0'][group]))\n",
    "print(\"Delta_eo1 = {}\".format(gaps_eo['eo_y1']))\n",
    "print(\"Delta_eo0 = {}\".format(gaps_eo['eo_y0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from sklearn import clone\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "class LogisticRegressionAsRegression(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, logistic_regression_estimator):\n",
    "        self.logistic_regression_estimator = logistic_regression_estimator\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        try:\n",
    "            check_is_fitted(self.logistic_regression_estimator)\n",
    "            self.logistic_regression_estimator_ = self.logistic_regression_estimator\n",
    "        except NotFittedError:\n",
    "            self.logistic_regression_estimator_ = clone(\n",
    "                self.logistic_regression_estimator\n",
    "            ).fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # use predict_proba to get real values instead of 0/1, select only prob for 1\n",
    "        scores = self.logistic_regression_estimator_.predict_proba(X)[:, 1]\n",
    "        return scores\n",
    "\n",
    "estimator_wrapper = LogisticRegressionAsRegression(logreg).fit(X_train, y_train)\n",
    "postprocessed_predictor_dp = ThresholdOptimizer(estimator=estimator_wrapper, constraints=\"demographic_parity\", prefit=True)\n",
    "postprocessed_predictor_eo = ThresholdOptimizer(estimator=estimator_wrapper, constraints=\"equalized_odds\", prefit=True)\n",
    "\n",
    "postprocessed_predictor_dp.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "postprocessed_predictor_eo.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "\n",
    "y_pred_dp = postprocessed_predictor_dp.predict(X_test, sensitive_features=sensitive_features_test)\n",
    "y_pred_eo = postprocessed_predictor_eo.predict(X_test, sensitive_features=sensitive_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPAS Test Accuracy (Postprocessing, DP): 0.6726315789473685\n",
      "COMPAS Test Accuracy (Postprocessing, EO): 0.6368421052631579\n"
     ]
    }
   ],
   "source": [
    "print(\"COMPAS Test Accuracy (Postprocessing, DP): {}\".format(accuracy_score(y_pred_dp, y_test)))\n",
    "print(\"COMPAS Test Accuracy (Postprocessing, EO): {}\".format(accuracy_score(y_pred_eo, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups, group_metrics_dp, gaps_dp = evaluate_fairness(y_test, y_pred_dp, sensitive_features_test)\n",
    "groups, group_metrics_eo, gaps_eo = evaluate_fairness(y_test, y_pred_eo, sensitive_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[h(X) = 1 | A = 0.0] = 0.4855072463768116\n",
      "P[h(X) = 1 | A = 1.0] = 0.45979899497487436\n",
      "Delta_dp = 0.02570825140193722\n",
      "P[h(X) = 1 | A = 0.0, Y = 1] = 0.7242647058823529\n",
      "P[h(X) = 1 | A = 0.0, Y = 0] = 0.4107142857142857\n",
      "P[h(X) = 1 | A = 1.0, Y = 1] = 0.7094594594594594\n",
      "P[h(X) = 1 | A = 1.0, Y = 0] = 0.448\n",
      "Delta_eo1 = 0.014805246422893492\n",
      "Delta_eo0 = 0.03728571428571431\n"
     ]
    }
   ],
   "source": [
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}] = {}\".format(group, group_metrics_dp['dp'][group]))\n",
    "print(\"Delta_dp = {}\".format(gaps_dp['dp']))\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 1] = {}\".format(group, group_metrics_eo['eo_y1'][group]))\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 0] = {}\".format(group, group_metrics_eo['eo_y0'][group]))\n",
    "print(\"Delta_eo1 = {}\".format(gaps_eo['eo_y1']))\n",
    "print(\"Delta_eo0 = {}\".format(gaps_eo['eo_y0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('./../../data/processed/adult/adult_train1_X.csv')\n",
    "X_test = pd.read_csv('./../../data/processed/adult/adult_test1_X.csv')\n",
    "y_train = pd.read_csv('./../../data/processed/adult/adult_train1_y.csv')\n",
    "y_train = y_train['income']\n",
    "y_test = pd.read_csv('./../../data/processed/adult/adult_test1_y.csv')\n",
    "y_test = y_test['income']\n",
    "\n",
    "sensitive_features_train = X_train['sex']\n",
    "sensitive_features_test = X_test['sex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult Test Accuracy (Unfair): 0.8341584158415841\n"
     ]
    }
   ],
   "source": [
    "print(\"Adult Test Accuracy (Unfair): {}\".format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups, group_metrics, gaps = evaluate_fairness(y_test, y_pred, sensitive_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[h(X) = 1 | A = -0.5903089186965219] = 0.5838926174496645\n",
      "P[h(X) = 1 | A = 1.6940282762593677] = 0.24528301886792453\n",
      "Delta_dp = 0.33860959858173995\n",
      "P[h(X) = 1 | A = -0.5903089186965219, Y = 1] = 0.8461538461538461\n",
      "P[h(X) = 1 | A = -0.5903089186965219, Y = 0] = 0.24031007751937986\n",
      "P[h(X) = 1 | A = 1.6940282762593677, Y = 1] = 0.8076923076923077\n",
      "P[h(X) = 1 | A = 1.6940282762593677, Y = 0] = 0.0625\n",
      "Delta_eo1 = 0.038461538461538436\n",
      "Delta_eo0 = 0.17781007751937986\n"
     ]
    }
   ],
   "source": [
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}] = {}\".format(group, group_metrics['dp'][group]))\n",
    "print(\"Delta_dp = {}\".format(gaps['dp']))\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 1] = {}\".format(group, group_metrics['eo_y1'][group]))\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 0] = {}\".format(group, group_metrics['eo_y0'][group]))\n",
    "print(\"Delta_eo1 = {}\".format(gaps['eo_y1']))\n",
    "print(\"Delta_eo0 = {}\".format(gaps['eo_y0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity, EqualizedOdds\n",
    "\n",
    "expgrad_dp = ExponentiatedGradient(\n",
    "    LogisticRegression(solver='liblinear', fit_intercept=True, class_weight='balanced'),\n",
    "    constraints=DemographicParity(),\n",
    "    eps=0.05,\n",
    "    nu=1e-6)\n",
    "\n",
    "expgrad_eo = ExponentiatedGradient(\n",
    "    LogisticRegression(solver='liblinear', fit_intercept=True, class_weight='balanced'),\n",
    "    constraints=EqualizedOdds(),\n",
    "    eps=0.05,\n",
    "    nu=1e-6)\n",
    "\n",
    "expgrad_dp.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "expgrad_eo.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "y_pred_dp = expgrad_dp.predict(X_test)\n",
    "y_pred_eo = expgrad_eo.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult Test Accuracy (Inprocessing, DP): 0.7970297029702971\n",
      "Adult Test Accuracy (Inprocessing, EO): 0.8242574257425742\n"
     ]
    }
   ],
   "source": [
    "print(\"Adult Test Accuracy (Inprocessing, DP): {}\".format(accuracy_score(y_pred_dp, y_test)))\n",
    "print(\"Adult Test Accuracy (Inprocessing, EO): {}\".format(accuracy_score(y_pred_eo, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups, group_metrics_dp, gaps_dp = evaluate_fairness(y_test, y_pred_dp, sensitive_features_test)\n",
    "groups, group_metrics_eo, gaps_eo = evaluate_fairness(y_test, y_pred_eo, sensitive_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[h(X) = 1 | A = -0.5903089186965219] = 0.5067114093959731\n",
      "P[h(X) = 1 | A = 1.6940282762593677] = 0.4528301886792453\n",
      "Delta_dp = 0.05388122071672785\n",
      "P[h(X) = 1 | A = -0.5903089186965219, Y = 1] = 0.8224852071005917\n",
      "P[h(X) = 1 | A = -0.5903089186965219, Y = 0] = 0.20155038759689922\n",
      "P[h(X) = 1 | A = 1.6940282762593677, Y = 1] = 0.8846153846153846\n",
      "P[h(X) = 1 | A = 1.6940282762593677, Y = 0] = 0.15\n",
      "Delta_eo1 = 0.062130177514792884\n",
      "Delta_eo0 = 0.05155038759689923\n"
     ]
    }
   ],
   "source": [
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}] = {}\".format(group, group_metrics_dp['dp'][group]))\n",
    "print(\"Delta_dp = {}\".format(gaps_dp['dp']))\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 1] = {}\".format(group, group_metrics_eo['eo_y1'][group]))\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 0] = {}\".format(group, group_metrics_eo['eo_y0'][group]))\n",
    "print(\"Delta_eo1 = {}\".format(gaps_eo['eo_y1']))\n",
    "print(\"Delta_eo0 = {}\".format(gaps_eo['eo_y0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from sklearn import clone\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "class LogisticRegressionAsRegression(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, logistic_regression_estimator):\n",
    "        self.logistic_regression_estimator = logistic_regression_estimator\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        try:\n",
    "            check_is_fitted(self.logistic_regression_estimator)\n",
    "            self.logistic_regression_estimator_ = self.logistic_regression_estimator\n",
    "        except NotFittedError:\n",
    "            self.logistic_regression_estimator_ = clone(\n",
    "                self.logistic_regression_estimator\n",
    "            ).fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # use predict_proba to get real values instead of 0/1, select only prob for 1\n",
    "        scores = self.logistic_regression_estimator_.predict_proba(X)[:, 1]\n",
    "        return scores\n",
    "\n",
    "estimator_wrapper = LogisticRegressionAsRegression(logreg).fit(X_train, y_train)\n",
    "postprocessed_predictor_dp = ThresholdOptimizer(estimator=estimator_wrapper, constraints=\"demographic_parity\", prefit=True)\n",
    "postprocessed_predictor_eo = ThresholdOptimizer(estimator=estimator_wrapper, constraints=\"equalized_odds\", prefit=True)\n",
    "\n",
    "postprocessed_predictor_dp.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "postprocessed_predictor_eo.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "\n",
    "y_pred_dp = postprocessed_predictor_dp.predict(X_test, sensitive_features=sensitive_features_test)\n",
    "y_pred_eo = postprocessed_predictor_eo.predict(X_test, sensitive_features=sensitive_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult Test Accuracy (Postprocessing, DP): 0.7846534653465347\n",
      "Adult Test Accuracy (Postprocessing, EO): 0.8044554455445545\n"
     ]
    }
   ],
   "source": [
    "print(\"Adult Test Accuracy (Postprocessing, DP): {}\".format(accuracy_score(y_pred_dp, y_test)))\n",
    "print(\"Adult Test Accuracy (Postprocessing, EO): {}\".format(accuracy_score(y_pred_eo, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups, group_metrics_dp, gaps_dp = evaluate_fairness(y_test, y_pred_dp, sensitive_features_test)\n",
    "groups, group_metrics_eo, gaps_eo = evaluate_fairness(y_test, y_pred_eo, sensitive_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[h(X) = 1 | A = -0.5903089186965219] = 0.5100671140939598\n",
      "P[h(X) = 1 | A = 1.6940282762593677] = 0.49056603773584906\n",
      "Delta_dp = 0.019501076358110714\n",
      "P[h(X) = 1 | A = -0.5903089186965219, Y = 1] = 0.8224852071005917\n",
      "P[h(X) = 1 | A = -0.5903089186965219, Y = 0] = 0.21705426356589147\n",
      "P[h(X) = 1 | A = 1.6940282762593677, Y = 1] = 0.8461538461538461\n",
      "P[h(X) = 1 | A = 1.6940282762593677, Y = 0] = 0.2125\n",
      "Delta_eo1 = 0.023668639053254448\n",
      "Delta_eo0 = 0.004554263565891475\n"
     ]
    }
   ],
   "source": [
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}] = {}\".format(group, group_metrics_dp['dp'][group]))\n",
    "print(\"Delta_dp = {}\".format(gaps_dp['dp']))\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 1] = {}\".format(group, group_metrics_eo['eo_y1'][group]))\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 0] = {}\".format(group, group_metrics_eo['eo_y0'][group]))\n",
    "print(\"Delta_eo1 = {}\".format(gaps_eo['eo_y1']))\n",
    "print(\"Delta_eo0 = {}\".format(gaps_eo['eo_y0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lawschool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('./../../data/processed/lawschool/lawschool_train1_X.csv')\n",
    "X_test = pd.read_csv('./../../data/processed/lawschool/lawschool_test1_X.csv')\n",
    "y_train = pd.read_csv('./../../data/processed/lawschool/lawschool_train1_y.csv')\n",
    "y_train = y_train['bar1']\n",
    "y_test = pd.read_csv('./../../data/processed/lawschool/lawschool_test1_y.csv')\n",
    "y_test = y_test['bar1']\n",
    "\n",
    "sensitive_features_train = X_train['race7']\n",
    "sensitive_features_test = X_test['race7']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/anaconda3/envs/fairness_checking/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lawschool Test Accuracy (Unfair): 0.8136986301369863\n"
     ]
    }
   ],
   "source": [
    "print(\"Lawschool Test Accuracy (Unfair): {}\".format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups, group_metrics, gaps = evaluate_fairness(y_test, y_pred, sensitive_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[h(X) = 1 | A = 0] = 0.22727272727272727\n",
      "P[h(X) = 1 | A = 1] = 0.6101083032490975\n",
      "Delta_dp = 0.3828355759763702\n",
      "P[h(X) = 1 | A = 0, Y = 1] = 0.6190476190476191\n",
      "P[h(X) = 1 | A = 0, Y = 0] = 0.1044776119402985\n",
      "P[h(X) = 1 | A = 1, Y = 1] = 0.8670886075949367\n",
      "P[h(X) = 1 | A = 1, Y = 0] = 0.2689075630252101\n",
      "Delta_eo1 = 0.2480409885473176\n",
      "Delta_eo0 = 0.16442995108491157\n"
     ]
    }
   ],
   "source": [
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}] = {}\".format(group, group_metrics['dp'][group]))\n",
    "print(\"Delta_dp = {}\".format(gaps['dp']))\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 1] = {}\".format(group, group_metrics['eo_y1'][group]))\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 0] = {}\".format(group, group_metrics['eo_y0'][group]))\n",
    "print(\"Delta_eo1 = {}\".format(gaps['eo_y1']))\n",
    "print(\"Delta_eo0 = {}\".format(gaps['eo_y0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity, EqualizedOdds\n",
    "\n",
    "expgrad_dp = ExponentiatedGradient(\n",
    "    LogisticRegression(solver='liblinear', fit_intercept=True, class_weight='balanced'),\n",
    "    constraints=DemographicParity(),\n",
    "    eps=0.05,\n",
    "    nu=1e-6)\n",
    "\n",
    "expgrad_eo = ExponentiatedGradient(\n",
    "    LogisticRegression(solver='liblinear', fit_intercept=True, class_weight='balanced'),\n",
    "    constraints=EqualizedOdds(),\n",
    "    eps=0.05,\n",
    "    nu=1e-6)\n",
    "\n",
    "expgrad_dp.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "expgrad_eo.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "y_pred_dp = expgrad_dp.predict(X_test)\n",
    "y_pred_eo = expgrad_eo.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lawschool Test Accuracy (Inprocessing, DP): 0.7452054794520548\n",
      "Lawschool Test Accuracy (Inprocessing, EO): 0.7753424657534247\n"
     ]
    }
   ],
   "source": [
    "print(\"Lawschool Test Accuracy (Inprocessing, DP): {}\".format(accuracy_score(y_pred_dp, y_test)))\n",
    "print(\"Lawschool Test Accuracy (Inprocessing, EO): {}\".format(accuracy_score(y_pred_eo, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups, group_metrics_dp, gaps_dp = evaluate_fairness(y_test, y_pred_dp, sensitive_features_test)\n",
    "groups, group_metrics_eo, gaps_eo = evaluate_fairness(y_test, y_pred_eo, sensitive_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[h(X) = 1 | A = 0] = 0.5681818181818182\n",
      "P[h(X) = 1 | A = 1] = 0.5342960288808665\n",
      "Delta_dp = 0.03388578930095176\n",
      "P[h(X) = 1 | A = 0, Y = 1] = 0.8095238095238095\n",
      "P[h(X) = 1 | A = 0, Y = 0] = 0.29850746268656714\n",
      "P[h(X) = 1 | A = 1, Y = 1] = 0.7848101265822784\n",
      "P[h(X) = 1 | A = 1, Y = 0] = 0.20168067226890757\n",
      "Delta_eo1 = 0.02471368294153109\n",
      "Delta_eo0 = 0.09682679041765957\n"
     ]
    }
   ],
   "source": [
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}] = {}\".format(group, group_metrics_dp['dp'][group]))\n",
    "print(\"Delta_dp = {}\".format(gaps_dp['dp']))\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 1] = {}\".format(group, group_metrics_eo['eo_y1'][group]))\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 0] = {}\".format(group, group_metrics_eo['eo_y0'][group]))\n",
    "print(\"Delta_eo1 = {}\".format(gaps_eo['eo_y1']))\n",
    "print(\"Delta_eo0 = {}\".format(gaps_eo['eo_y0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from sklearn import clone\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "class LogisticRegressionAsRegression(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, logistic_regression_estimator):\n",
    "        self.logistic_regression_estimator = logistic_regression_estimator\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        try:\n",
    "            check_is_fitted(self.logistic_regression_estimator)\n",
    "            self.logistic_regression_estimator_ = self.logistic_regression_estimator\n",
    "        except NotFittedError:\n",
    "            self.logistic_regression_estimator_ = clone(\n",
    "                self.logistic_regression_estimator\n",
    "            ).fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # use predict_proba to get real values instead of 0/1, select only prob for 1\n",
    "        scores = self.logistic_regression_estimator_.predict_proba(X)[:, 1]\n",
    "        return scores\n",
    "\n",
    "estimator_wrapper = LogisticRegressionAsRegression(logreg).fit(X_train, y_train)\n",
    "postprocessed_predictor_dp = ThresholdOptimizer(estimator=estimator_wrapper, constraints=\"demographic_parity\", prefit=True)\n",
    "postprocessed_predictor_eo = ThresholdOptimizer(estimator=estimator_wrapper, constraints=\"equalized_odds\", prefit=True)\n",
    "\n",
    "postprocessed_predictor_dp.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "postprocessed_predictor_eo.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "\n",
    "y_pred_dp = postprocessed_predictor_dp.predict(X_test, sensitive_features=sensitive_features_test)\n",
    "y_pred_eo = postprocessed_predictor_eo.predict(X_test, sensitive_features=sensitive_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lawschool Test Accuracy (Postprocessing, DP): 0.7643835616438356\n",
      "Lawschool Test Accuracy (Postprocessing, EO): 0.7315068493150685\n"
     ]
    }
   ],
   "source": [
    "print(\"Lawschool Test Accuracy (Postprocessing, DP): {}\".format(accuracy_score(y_pred_dp, y_test)))\n",
    "print(\"Lawschool Test Accuracy (Postprocessing, EO): {}\".format(accuracy_score(y_pred_eo, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups, group_metrics_dp, gaps_dp = evaluate_fairness(y_test, y_pred_dp, sensitive_features_test)\n",
    "groups, group_metrics_eo, gaps_eo = evaluate_fairness(y_test, y_pred_eo, sensitive_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[h(X) = 1 | A = 0] = 0.5227272727272727\n",
      "P[h(X) = 1 | A = 1] = 0.5451263537906137\n",
      "Delta_dp = 0.022399081063340986\n",
      "P[h(X) = 1 | A = 0, Y = 1] = 0.7619047619047619\n",
      "P[h(X) = 1 | A = 0, Y = 0] = 0.2537313432835821\n",
      "P[h(X) = 1 | A = 1, Y = 1] = 0.6708860759493671\n",
      "P[h(X) = 1 | A = 1, Y = 0] = 0.20168067226890757\n",
      "Delta_eo1 = 0.09101868595539475\n",
      "Delta_eo0 = 0.05205067101467453\n"
     ]
    }
   ],
   "source": [
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}] = {}\".format(group, group_metrics_dp['dp'][group]))\n",
    "print(\"Delta_dp = {}\".format(gaps_dp['dp']))\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 1] = {}\".format(group, group_metrics_eo['eo_y1'][group]))\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 0] = {}\".format(group, group_metrics_eo['eo_y0'][group]))\n",
    "print(\"Delta_eo1 = {}\".format(gaps_eo['eo_y1']))\n",
    "print(\"Delta_eo0 = {}\".format(gaps_eo['eo_y0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('./../../data/processed/communities/communities_train1_X.csv')\n",
    "X_test = pd.read_csv('./../../data/processed/communities/communities_test1_X.csv')\n",
    "y_train = pd.read_csv('./../../data/processed/communities/communities_train1_y.csv')\n",
    "y_train = y_train['ViolentCrimesPerPop']\n",
    "y_test = pd.read_csv('./../../data/processed/communities/communities_test1_y.csv')\n",
    "y_test = y_test['ViolentCrimesPerPop']\n",
    "\n",
    "sensitive_features_train = X_train['majority_white']\n",
    "sensitive_features_test = X_test['majority_white']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/anaconda3/envs/fairness_checking/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communities Test Accuracy (Unfair): 0.8646616541353384\n"
     ]
    }
   ],
   "source": [
    "print(\"Communities Test Accuracy (Unfair): {}\".format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[h(X) = 1 | A = 0] = 0.6379310344827587\n",
      "P[h(X) = 1 | A = 1] = 0.09187279151943463\n",
      "Delta_dp = 0.5460582429633241\n",
      "P[h(X) = 1 | A = 0, Y = 1] = 0.8026315789473685\n",
      "P[h(X) = 1 | A = 0, Y = 0] = 0.325\n",
      "P[h(X) = 1 | A = 1, Y = 1] = 0.5\n",
      "P[h(X) = 1 | A = 1, Y = 0] = 0.024691358024691357\n",
      "Delta_eo1 = 0.3026315789473685\n",
      "Delta_eo0 = 0.30030864197530865\n"
     ]
    }
   ],
   "source": [
    "groups, group_metrics, gaps = evaluate_fairness(y_test, y_pred, sensitive_features_test)\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}] = {}\".format(group, group_metrics['dp'][group]))\n",
    "print(\"Delta_dp = {}\".format(gaps['dp']))\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 1] = {}\".format(group, group_metrics['eo_y1'][group]))\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 0] = {}\".format(group, group_metrics['eo_y0'][group]))\n",
    "print(\"Delta_eo1 = {}\".format(gaps['eo_y1']))\n",
    "print(\"Delta_eo0 = {}\".format(gaps['eo_y0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity, EqualizedOdds\n",
    "\n",
    "expgrad_dp = ExponentiatedGradient(\n",
    "    LogisticRegression(solver='liblinear', fit_intercept=True, class_weight='balanced'),\n",
    "    constraints=DemographicParity(),\n",
    "    eps=0.05,\n",
    "    nu=1e-6)\n",
    "\n",
    "expgrad_eo = ExponentiatedGradient(\n",
    "    LogisticRegression(solver='liblinear', fit_intercept=True, class_weight='balanced'),\n",
    "    constraints=EqualizedOdds(),\n",
    "    eps=0.05,\n",
    "    nu=1e-6)\n",
    "\n",
    "expgrad_dp.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "expgrad_eo.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "y_pred_dp = expgrad_dp.predict(X_test)\n",
    "y_pred_eo = expgrad_eo.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communities Test Accuracy (Inprocessing, DP): 0.7268170426065163\n",
      "Communities Test Accuracy (Inprocessing, EO): 0.8120300751879699\n"
     ]
    }
   ],
   "source": [
    "print(\"Communities Test Accuracy (Inprocessing, DP): {}\".format(accuracy_score(y_pred_dp, y_test)))\n",
    "print(\"Communities Test Accuracy (Inprocessing, EO): {}\".format(accuracy_score(y_pred_eo, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[h(X) = 1 | A = 0] = 0.41379310344827586\n",
      "P[h(X) = 1 | A = 1] = 0.34275618374558303\n",
      "Delta_dp = 0.07103691970269282\n",
      "P[h(X) = 1 | A = 0, Y = 1] = 0.75\n",
      "P[h(X) = 1 | A = 0, Y = 0] = 0.275\n",
      "P[h(X) = 1 | A = 1, Y = 1] = 0.8\n",
      "P[h(X) = 1 | A = 1, Y = 0] = 0.1522633744855967\n",
      "Delta_eo1 = 0.050000000000000044\n",
      "Delta_eo0 = 0.12273662551440331\n"
     ]
    }
   ],
   "source": [
    "groups, group_metrics_dp, gaps_dp = evaluate_fairness(y_test, y_pred_dp, sensitive_features_test)\n",
    "groups, group_metrics_eo, gaps_eo = evaluate_fairness(y_test, y_pred_eo, sensitive_features_test)\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}] = {}\".format(group, group_metrics_dp['dp'][group]))\n",
    "print(\"Delta_dp = {}\".format(gaps_dp['dp']))\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 1] = {}\".format(group, group_metrics_eo['eo_y1'][group]))\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 0] = {}\".format(group, group_metrics_eo['eo_y0'][group]))\n",
    "print(\"Delta_eo1 = {}\".format(gaps_eo['eo_y1']))\n",
    "print(\"Delta_eo0 = {}\".format(gaps_eo['eo_y0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from sklearn import clone\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "class LogisticRegressionAsRegression(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, logistic_regression_estimator):\n",
    "        self.logistic_regression_estimator = logistic_regression_estimator\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        try:\n",
    "            check_is_fitted(self.logistic_regression_estimator)\n",
    "            self.logistic_regression_estimator_ = self.logistic_regression_estimator\n",
    "        except NotFittedError:\n",
    "            self.logistic_regression_estimator_ = clone(\n",
    "                self.logistic_regression_estimator\n",
    "            ).fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # use predict_proba to get real values instead of 0/1, select only prob for 1\n",
    "        scores = self.logistic_regression_estimator_.predict_proba(X)[:, 1]\n",
    "        return scores\n",
    "\n",
    "estimator_wrapper = LogisticRegressionAsRegression(logreg).fit(X_train, y_train)\n",
    "postprocessed_predictor_dp = ThresholdOptimizer(estimator=estimator_wrapper, constraints=\"demographic_parity\", prefit=True)\n",
    "postprocessed_predictor_eo = ThresholdOptimizer(estimator=estimator_wrapper, constraints=\"equalized_odds\", prefit=True)\n",
    "\n",
    "postprocessed_predictor_dp.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "postprocessed_predictor_eo.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "\n",
    "y_pred_dp = postprocessed_predictor_dp.predict(X_test, sensitive_features=sensitive_features_test)\n",
    "y_pred_eo = postprocessed_predictor_eo.predict(X_test, sensitive_features=sensitive_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communities Test Accuracy (Postprocessing, DP): 0.7593984962406015\n",
      "Communities Test Accuracy (Postprocessing, EO): 0.8295739348370927\n"
     ]
    }
   ],
   "source": [
    "print(\"Communities Test Accuracy (Postprocessing, DP): {}\".format(accuracy_score(y_pred_dp, y_test)))\n",
    "print(\"Communities Test Accuracy (Postprocessing, EO): {}\".format(accuracy_score(y_pred_eo, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[h(X) = 1 | A = 0] = 0.1724137931034483\n",
      "P[h(X) = 1 | A = 1] = 0.2049469964664311\n",
      "Delta_dp = 0.03253320336298282\n",
      "P[h(X) = 1 | A = 0, Y = 1] = 0.6710526315789473\n",
      "P[h(X) = 1 | A = 0, Y = 0] = 0.2\n",
      "P[h(X) = 1 | A = 1, Y = 1] = 0.75\n",
      "P[h(X) = 1 | A = 1, Y = 0] = 0.102880658436214\n",
      "Delta_eo1 = 0.07894736842105265\n",
      "Delta_eo0 = 0.09711934156378602\n"
     ]
    }
   ],
   "source": [
    "groups, group_metrics_dp, gaps_dp = evaluate_fairness(y_test, y_pred_dp, sensitive_features_test)\n",
    "groups, group_metrics_eo, gaps_eo = evaluate_fairness(y_test, y_pred_eo, sensitive_features_test)\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}] = {}\".format(group, group_metrics_dp['dp'][group]))\n",
    "print(\"Delta_dp = {}\".format(gaps_dp['dp']))\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 1] = {}\".format(group, group_metrics_eo['eo_y1'][group]))\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 0] = {}\".format(group, group_metrics_eo['eo_y0'][group]))\n",
    "print(\"Delta_eo1 = {}\".format(gaps_eo['eo_y1']))\n",
    "print(\"Delta_eo0 = {}\".format(gaps_eo['eo_y0']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fairness_checking] *",
   "language": "python",
   "name": "conda-env-fairness_checking-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
