{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we find the baseline fairness using each of the three fairness algorithms we would like to compare: Reweighing (Preprocessing), Reductions Approach to Fair Classficiation (Inprocessing), and Equality of Opportunity in Supervised Learning (Postprocessing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def evaluate_fairness(y_true, y_pred, sensitive_features):\n",
    "    \"\"\"\n",
    "    Evaluates fairness of the final majority vote classifier over T_inner hypotheses\n",
    "    on the test set.\n",
    "    #NOTE: defined in the meta_algo file, but we chose:\n",
    "    a0 := African-American (COMPAS), Female (Adult)\n",
    "    a1 := Caucasian (COMPAS), Male (Adult)\n",
    "\n",
    "    :return: list. subgroups in sensitive_features.\n",
    "    :return: list, dict, dict. groups is a list of the sensitive features in the dataset. \n",
    "    group_metrics is a dictionary containing dictionaries that have Delta_dp, Delta_eoy0, \n",
    "    and Delta_eoy1 for each group. gaps is a dictionary that contains the fairness gap\n",
    "    for dp, eo_y0 and eo_y1.\n",
    "    \"\"\"\n",
    "    groups = np.unique(sensitive_features.values)\n",
    "    pos_count = {}\n",
    "    dp_pct = {}\n",
    "    eo_y0_pct = {}\n",
    "    eo_y1_pct = {}\n",
    "\n",
    "    for index, group in enumerate(groups):\n",
    "        # Demographic Parity\n",
    "        indices = {}\n",
    "        indices[group] = sensitive_features.index[sensitive_features == group]\n",
    "        dp_pct[group] = sum(y_pred[indices[group]])/len(indices[group])\n",
    "\n",
    "        # Equalized Odds\n",
    "        y1_indices = {}\n",
    "        y0_indices = {}\n",
    "        y1_indices[group] = sensitive_features.index[(sensitive_features == group) & (y_true == 1)]\n",
    "        y0_indices[group] = sensitive_features.index[(sensitive_features == group) & (y_true == 0)]\n",
    "        eo_y0_pct[group] = sum(y_pred[y0_indices[group]])/len(y0_indices[group])   \n",
    "        eo_y1_pct[group] = sum(y_pred[y1_indices[group]])/len(y1_indices[group])\n",
    "\n",
    "    gaps = {}\n",
    "    group_metrics = {} # a dictionary of dictionaries\n",
    "\n",
    "    gaps['dp'] = abs(dp_pct[groups[0]] - dp_pct[groups[1]])\n",
    "    gaps['eo_y0'] = abs(eo_y0_pct[groups[0]] - eo_y0_pct[groups[1]])\n",
    "    gaps['eo_y1'] = abs(eo_y1_pct[groups[0]] - eo_y1_pct[groups[1]])\n",
    "    group_metrics['dp'] = dp_pct\n",
    "    group_metrics['eo_y0'] = eo_y0_pct\n",
    "    group_metrics['eo_y1'] = eo_y1_pct\n",
    "\n",
    "    return groups, group_metrics, gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('./../../data/processed/compas/compas_train1_X.csv')\n",
    "X_test = pd.read_csv('./../../data/processed/compas/compas_test1_X.csv')\n",
    "y_train = pd.read_csv('./../../data/processed/compas/compas_train1_y.csv')\n",
    "y_train = y_train['two_year_recid']\n",
    "y_test = pd.read_csv('./../../data/processed/compas/compas_test1_y.csv')\n",
    "y_test = y_test['two_year_recid']\n",
    "\n",
    "sensitive_features_train = X_train['race']\n",
    "sensitive_features_test = X_test['race']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPAS Test Accuracy (Unfair): 0.6926315789473684\n"
     ]
    }
   ],
   "source": [
    "print(\"COMPAS Test Accuracy (Unfair): {}\".format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups, group_metrics, gaps = evaluate_fairness(y_test, y_pred, sensitive_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[h(X) = 1 | A = 0.0] = 0.4963768115942029\n",
      "P[h(X) = 1 | A = 1.0] = 0.24623115577889448\n",
      "Delta_dp = 0.2501456558153084\n",
      "P[h(X) = 1 | A = 0.0, Y = 1] = 0.6948529411764706\n",
      "P[h(X) = 1 | A = 0.0, Y = 0] = 0.30357142857142855\n",
      "P[h(X) = 1 | A = 1.0, Y = 1] = 0.41216216216216217\n",
      "P[h(X) = 1 | A = 1.0, Y = 0] = 0.148\n",
      "Delta_eo1 = 0.2826907790143084\n",
      "Delta_eo0 = 0.15557142857142855\n"
     ]
    }
   ],
   "source": [
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}] = {}\".format(group, group_metrics['dp'][group]))\n",
    "print(\"Delta_dp = {}\".format(gaps['dp']))\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 1] = {}\".format(group, group_metrics['eo_y1'][group]))\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 0] = {}\".format(group, group_metrics['eo_y0'][group]))\n",
    "print(\"Delta_eo1 = {}\".format(gaps['eo_y1']))\n",
    "print(\"Delta_eo0 = {}\".format(gaps['eo_y0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity, EqualizedOdds\n",
    "\n",
    "expgrad_dp = ExponentiatedGradient(\n",
    "    LogisticRegression(solver='liblinear', fit_intercept=True, class_weight='balanced'),\n",
    "    constraints=DemographicParity(),\n",
    "    eps=0.05,\n",
    "    nu=1e-6)\n",
    "\n",
    "expgrad_eo = ExponentiatedGradient(\n",
    "    LogisticRegression(solver='liblinear', fit_intercept=True, class_weight='balanced'),\n",
    "    constraints=EqualizedOdds(),\n",
    "    eps=0.05,\n",
    "    nu=1e-6)\n",
    "\n",
    "expgrad_dp.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "expgrad_eo.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "y_pred_dp = expgrad_dp.predict(X_test)\n",
    "y_pred_eo = expgrad_eo.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPAS Test Accuracy (Inprocessing, DP): 0.68\n",
      "COMPAS Test Accuracy (Inprocessing, EO): 0.6757894736842105\n"
     ]
    }
   ],
   "source": [
    "print(\"COMPAS Test Accuracy (Inprocessing, DP): {}\".format(accuracy_score(y_pred_dp, y_test)))\n",
    "print(\"COMPAS Test Accuracy (Inprocessing, EO): {}\".format(accuracy_score(y_pred_eo, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups, group_metrics_dp, gaps_dp = evaluate_fairness(y_test, y_pred_dp, sensitive_features_test)\n",
    "groups, group_metrics_eo, gaps_eo = evaluate_fairness(y_test, y_pred_eo, sensitive_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[h(X) = 1 | A = 0.0] = 0.49818840579710144\n",
      "P[h(X) = 1 | A = 1.0] = 0.39949748743718594\n",
      "Delta_dp = 0.0986909183599155\n",
      "P[h(X) = 1 | A = 0.0, Y = 1] = 0.6948529411764706\n",
      "P[h(X) = 1 | A = 0.0, Y = 0] = 0.32142857142857145\n",
      "P[h(X) = 1 | A = 1.0, Y = 1] = 0.5743243243243243\n",
      "P[h(X) = 1 | A = 1.0, Y = 0] = 0.288\n",
      "Delta_eo1 = 0.12052861685214622\n",
      "Delta_eo0 = 0.033428571428571474\n"
     ]
    }
   ],
   "source": [
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}] = {}\".format(group, group_metrics_dp['dp'][group]))\n",
    "print(\"Delta_dp = {}\".format(gaps_dp['dp']))\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 1] = {}\".format(group, group_metrics_eo['eo_y1'][group]))\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 0] = {}\".format(group, group_metrics_eo['eo_y0'][group]))\n",
    "print(\"Delta_eo1 = {}\".format(gaps_eo['eo_y1']))\n",
    "print(\"Delta_eo0 = {}\".format(gaps_eo['eo_y0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from sklearn import clone\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "class LogisticRegressionAsRegression(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, logistic_regression_estimator):\n",
    "        self.logistic_regression_estimator = logistic_regression_estimator\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        try:\n",
    "            check_is_fitted(self.logistic_regression_estimator)\n",
    "            self.logistic_regression_estimator_ = self.logistic_regression_estimator\n",
    "        except NotFittedError:\n",
    "            self.logistic_regression_estimator_ = clone(\n",
    "                self.logistic_regression_estimator\n",
    "            ).fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # use predict_proba to get real values instead of 0/1, select only prob for 1\n",
    "        scores = self.logistic_regression_estimator_.predict_proba(X)[:, 1]\n",
    "        return scores\n",
    "\n",
    "estimator_wrapper = LogisticRegressionAsRegression(logreg).fit(X_train, y_train)\n",
    "postprocessed_predictor_dp = ThresholdOptimizer(estimator=estimator_wrapper, constraints=\"demographic_parity\", prefit=True)\n",
    "postprocessed_predictor_eo = ThresholdOptimizer(estimator=estimator_wrapper, constraints=\"equalized_odds\", prefit=True)\n",
    "\n",
    "postprocessed_predictor_dp.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "postprocessed_predictor_eo.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "\n",
    "y_pred_dp = postprocessed_predictor_dp.predict(X_test, sensitive_features=sensitive_features_test)\n",
    "y_pred_eo = postprocessed_predictor_eo.predict(X_test, sensitive_features=sensitive_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPAS Test Accuracy (Postprocessing, DP): 0.6736842105263158\n",
      "COMPAS Test Accuracy (Postprocessing, EO): 0.6357894736842106\n"
     ]
    }
   ],
   "source": [
    "print(\"COMPAS Test Accuracy (Postprocessing, DP): {}\".format(accuracy_score(y_pred_dp, y_test)))\n",
    "print(\"COMPAS Test Accuracy (Postprocessing, EO): {}\".format(accuracy_score(y_pred_eo, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups, group_metrics_dp, gaps_dp = evaluate_fairness(y_test, y_pred_dp, sensitive_features_test)\n",
    "groups, group_metrics_eo, gaps_eo = evaluate_fairness(y_test, y_pred_eo, sensitive_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[h(X) = 1 | A = 0.0] = 0.4855072463768116\n",
      "P[h(X) = 1 | A = 1.0] = 0.46733668341708545\n",
      "Delta_dp = 0.018170562959726133\n",
      "P[h(X) = 1 | A = 0.0, Y = 1] = 0.7205882352941176\n",
      "P[h(X) = 1 | A = 0.0, Y = 0] = 0.4107142857142857\n",
      "P[h(X) = 1 | A = 1.0, Y = 1] = 0.7094594594594594\n",
      "P[h(X) = 1 | A = 1.0, Y = 0] = 0.448\n",
      "Delta_eo1 = 0.011128775834658211\n",
      "Delta_eo0 = 0.03728571428571431\n"
     ]
    }
   ],
   "source": [
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}] = {}\".format(group, group_metrics_dp['dp'][group]))\n",
    "print(\"Delta_dp = {}\".format(gaps_dp['dp']))\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 1] = {}\".format(group, group_metrics_eo['eo_y1'][group]))\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 0] = {}\".format(group, group_metrics_eo['eo_y0'][group]))\n",
    "print(\"Delta_eo1 = {}\".format(gaps_eo['eo_y1']))\n",
    "print(\"Delta_eo0 = {}\".format(gaps_eo['eo_y0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('./../../data/processed/adult/adult_train1_X.csv')\n",
    "X_test = pd.read_csv('./../../data/processed/adult/adult_test1_X.csv')\n",
    "y_train = pd.read_csv('./../../data/processed/adult/adult_train1_y.csv')\n",
    "y_train = y_train['income']\n",
    "y_test = pd.read_csv('./../../data/processed/adult/adult_test1_y.csv')\n",
    "y_test = y_test['income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_features_train = X_train['sex']\n",
    "sensitive_features_test = X_test['sex']\n",
    "\n",
    "sensitive_features_train[sensitive_features_train < 0] = 0\n",
    "sensitive_features_train[sensitive_features_train > 0] = 1\n",
    "sensitive_features_train = sensitive_features_train.reset_index(drop=True)\n",
    "\n",
    "sensitive_features_test[sensitive_features_test < 0] = 0\n",
    "sensitive_features_test[sensitive_features_test > 0] = 1\n",
    "sensitive_features_test = sensitive_features_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult Test Accuracy (Unfair): 0.8316831683168316\n"
     ]
    }
   ],
   "source": [
    "print(\"Adult Test Accuracy (Unfair): {}\".format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups, group_metrics, gaps = evaluate_fairness(y_test, y_pred, sensitive_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[h(X) = 1 | A = 0.0] = 0.587248322147651\n",
      "P[h(X) = 1 | A = 1.0] = 0.24528301886792453\n",
      "Delta_dp = 0.34196530327972646\n",
      "P[h(X) = 1 | A = 0.0, Y = 1] = 0.8461538461538461\n",
      "P[h(X) = 1 | A = 0.0, Y = 0] = 0.24806201550387597\n",
      "P[h(X) = 1 | A = 1.0, Y = 1] = 0.8076923076923077\n",
      "P[h(X) = 1 | A = 1.0, Y = 0] = 0.0625\n",
      "Delta_eo1 = 0.038461538461538436\n",
      "Delta_eo0 = 0.18556201550387597\n"
     ]
    }
   ],
   "source": [
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}] = {}\".format(group, group_metrics['dp'][group]))\n",
    "print(\"Delta_dp = {}\".format(gaps['dp']))\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 1] = {}\".format(group, group_metrics['eo_y1'][group]))\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 0] = {}\".format(group, group_metrics['eo_y0'][group]))\n",
    "print(\"Delta_eo1 = {}\".format(gaps['eo_y1']))\n",
    "print(\"Delta_eo0 = {}\".format(gaps['eo_y0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity, EqualizedOdds\n",
    "\n",
    "expgrad_dp = ExponentiatedGradient(\n",
    "    LogisticRegression(solver='liblinear', fit_intercept=True, class_weight='balanced'),\n",
    "    constraints=DemographicParity(),\n",
    "    eps=0.05,\n",
    "    nu=1e-6)\n",
    "\n",
    "expgrad_eo = ExponentiatedGradient(\n",
    "    LogisticRegression(solver='liblinear', fit_intercept=True, class_weight='balanced'),\n",
    "    constraints=EqualizedOdds(),\n",
    "    eps=0.05,\n",
    "    nu=1e-6)\n",
    "\n",
    "expgrad_dp.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "expgrad_eo.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "y_pred_dp = expgrad_dp.predict(X_test)\n",
    "y_pred_eo = expgrad_eo.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult Test Accuracy (Inprocessing, DP): 0.7995049504950495\n",
      "Adult Test Accuracy (Inprocessing, EO): 0.8217821782178217\n"
     ]
    }
   ],
   "source": [
    "print(\"Adult Test Accuracy (Inprocessing, DP): {}\".format(accuracy_score(y_pred_dp, y_test)))\n",
    "print(\"Adult Test Accuracy (Inprocessing, EO): {}\".format(accuracy_score(y_pred_eo, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups, group_metrics_dp, gaps_dp = evaluate_fairness(y_test, y_pred_dp, sensitive_features_test)\n",
    "groups, group_metrics_eo, gaps_eo = evaluate_fairness(y_test, y_pred_eo, sensitive_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[h(X) = 1 | A = 0.0] = 0.5100671140939598\n",
      "P[h(X) = 1 | A = 1.0] = 0.4528301886792453\n",
      "Delta_dp = 0.05723692541471448\n",
      "P[h(X) = 1 | A = 0.0, Y = 1] = 0.8106508875739645\n",
      "P[h(X) = 1 | A = 0.0, Y = 0] = 0.20155038759689922\n",
      "P[h(X) = 1 | A = 1.0, Y = 1] = 0.9615384615384616\n",
      "P[h(X) = 1 | A = 1.0, Y = 0] = 0.1625\n",
      "Delta_eo1 = 0.15088757396449703\n",
      "Delta_eo0 = 0.039050387596899216\n"
     ]
    }
   ],
   "source": [
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}] = {}\".format(group, group_metrics_dp['dp'][group]))\n",
    "print(\"Delta_dp = {}\".format(gaps_dp['dp']))\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 1] = {}\".format(group, group_metrics_eo['eo_y1'][group]))\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 0] = {}\".format(group, group_metrics_eo['eo_y0'][group]))\n",
    "print(\"Delta_eo1 = {}\".format(gaps_eo['eo_y1']))\n",
    "print(\"Delta_eo0 = {}\".format(gaps_eo['eo_y0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from sklearn import clone\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "class LogisticRegressionAsRegression(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, logistic_regression_estimator):\n",
    "        self.logistic_regression_estimator = logistic_regression_estimator\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        try:\n",
    "            check_is_fitted(self.logistic_regression_estimator)\n",
    "            self.logistic_regression_estimator_ = self.logistic_regression_estimator\n",
    "        except NotFittedError:\n",
    "            self.logistic_regression_estimator_ = clone(\n",
    "                self.logistic_regression_estimator\n",
    "            ).fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # use predict_proba to get real values instead of 0/1, select only prob for 1\n",
    "        scores = self.logistic_regression_estimator_.predict_proba(X)[:, 1]\n",
    "        return scores\n",
    "\n",
    "estimator_wrapper = LogisticRegressionAsRegression(logreg).fit(X_train, y_train)\n",
    "postprocessed_predictor_dp = ThresholdOptimizer(estimator=estimator_wrapper, constraints=\"demographic_parity\", prefit=True)\n",
    "postprocessed_predictor_eo = ThresholdOptimizer(estimator=estimator_wrapper, constraints=\"equalized_odds\", prefit=True)\n",
    "\n",
    "postprocessed_predictor_dp.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "postprocessed_predictor_eo.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "\n",
    "y_pred_dp = postprocessed_predictor_dp.predict(X_test, sensitive_features=sensitive_features_test)\n",
    "y_pred_eo = postprocessed_predictor_eo.predict(X_test, sensitive_features=sensitive_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult Test Accuracy (Postprocessing, DP): 0.7871287128712872\n",
      "Adult Test Accuracy (Postprocessing, EO): 0.8168316831683168\n"
     ]
    }
   ],
   "source": [
    "print(\"Adult Test Accuracy (Postprocessing, DP): {}\".format(accuracy_score(y_pred_dp, y_test)))\n",
    "print(\"Adult Test Accuracy (Postprocessing, EO): {}\".format(accuracy_score(y_pred_eo, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups, group_metrics_dp, gaps_dp = evaluate_fairness(y_test, y_pred_dp, sensitive_features_test)\n",
    "groups, group_metrics_eo, gaps_eo = evaluate_fairness(y_test, y_pred_eo, sensitive_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[h(X) = 1 | A = 0.0] = 0.5100671140939598\n",
      "P[h(X) = 1 | A = 1.0] = 0.4811320754716981\n",
      "Delta_dp = 0.028935038622261655\n",
      "P[h(X) = 1 | A = 0.0, Y = 1] = 0.834319526627219\n",
      "P[h(X) = 1 | A = 0.0, Y = 0] = 0.20930232558139536\n",
      "P[h(X) = 1 | A = 1.0, Y = 1] = 0.9230769230769231\n",
      "P[h(X) = 1 | A = 1.0, Y = 0] = 0.2125\n",
      "Delta_eo1 = 0.08875739644970415\n",
      "Delta_eo0 = 0.0031976744186046346\n"
     ]
    }
   ],
   "source": [
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}] = {}\".format(group, group_metrics_dp['dp'][group]))\n",
    "print(\"Delta_dp = {}\".format(gaps_dp['dp']))\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 1] = {}\".format(group, group_metrics_eo['eo_y1'][group]))\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 0] = {}\".format(group, group_metrics_eo['eo_y0'][group]))\n",
    "print(\"Delta_eo1 = {}\".format(gaps_eo['eo_y1']))\n",
    "print(\"Delta_eo0 = {}\".format(gaps_eo['eo_y0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lawschool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/anaconda3/envs/fairness_checking/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/sam/anaconda3/envs/fairness_checking/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "/Users/sam/anaconda3/envs/fairness_checking/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/sam/anaconda3/envs/fairness_checking/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv('./../../data/processed/lawschool/lawschool_train1_X.csv')\n",
    "X_test = pd.read_csv('./../../data/processed/lawschool/lawschool_test1_X.csv')\n",
    "y_train = pd.read_csv('./../../data/processed/lawschool/lawschool_train1_y.csv')\n",
    "y_train = y_train['bar1']\n",
    "y_test = pd.read_csv('./../../data/processed/lawschool/lawschool_test1_y.csv')\n",
    "y_test = y_test['bar1']\n",
    "\n",
    "sensitive_features_train = X_train['race7']\n",
    "sensitive_features_test = X_test['race7']\n",
    "\n",
    "sensitive_features_train[sensitive_features_train < 0] = 0\n",
    "sensitive_features_train[sensitive_features_train > 0] = 1\n",
    "sensitive_features_train = sensitive_features_train.reset_index(drop=True)\n",
    "\n",
    "sensitive_features_test[sensitive_features_test < 0] = 0\n",
    "sensitive_features_test[sensitive_features_test > 0] = 1\n",
    "sensitive_features_test = sensitive_features_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/anaconda3/envs/fairness_checking/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lawschool Test Accuracy (Unfair): 0.8136986301369863\n"
     ]
    }
   ],
   "source": [
    "print(\"Lawschool Test Accuracy (Unfair): {}\".format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups, group_metrics, gaps = evaluate_fairness(y_test, y_pred, sensitive_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[h(X) = 1 | A = 0] = 0.22727272727272727\n",
      "P[h(X) = 1 | A = 1] = 0.6101083032490975\n",
      "Delta_dp = 0.3828355759763702\n",
      "P[h(X) = 1 | A = 0, Y = 1] = 0.6190476190476191\n",
      "P[h(X) = 1 | A = 0, Y = 0] = 0.1044776119402985\n",
      "P[h(X) = 1 | A = 1, Y = 1] = 0.8670886075949367\n",
      "P[h(X) = 1 | A = 1, Y = 0] = 0.2689075630252101\n",
      "Delta_eo1 = 0.2480409885473176\n",
      "Delta_eo0 = 0.16442995108491157\n"
     ]
    }
   ],
   "source": [
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}] = {}\".format(group, group_metrics['dp'][group]))\n",
    "print(\"Delta_dp = {}\".format(gaps['dp']))\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 1] = {}\".format(group, group_metrics['eo_y1'][group]))\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 0] = {}\".format(group, group_metrics['eo_y0'][group]))\n",
    "print(\"Delta_eo1 = {}\".format(gaps['eo_y1']))\n",
    "print(\"Delta_eo0 = {}\".format(gaps['eo_y0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity, EqualizedOdds\n",
    "\n",
    "expgrad_dp = ExponentiatedGradient(\n",
    "    LogisticRegression(solver='liblinear', fit_intercept=True, class_weight='balanced'),\n",
    "    constraints=DemographicParity(),\n",
    "    eps=0.05,\n",
    "    nu=1e-6)\n",
    "\n",
    "expgrad_eo = ExponentiatedGradient(\n",
    "    LogisticRegression(solver='liblinear', fit_intercept=True, class_weight='balanced'),\n",
    "    constraints=EqualizedOdds(),\n",
    "    eps=0.05,\n",
    "    nu=1e-6)\n",
    "\n",
    "expgrad_dp.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "expgrad_eo.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "y_pred_dp = expgrad_dp.predict(X_test)\n",
    "y_pred_eo = expgrad_eo.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lawschool Test Accuracy (Inprocessing, DP): 0.7506849315068493\n",
      "Lawschool Test Accuracy (Inprocessing, EO): 0.7671232876712328\n"
     ]
    }
   ],
   "source": [
    "print(\"Lawschool Test Accuracy (Inprocessing, DP): {}\".format(accuracy_score(y_pred_dp, y_test)))\n",
    "print(\"Lawschool Test Accuracy (Inprocessing, EO): {}\".format(accuracy_score(y_pred_eo, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups, group_metrics_dp, gaps_dp = evaluate_fairness(y_test, y_pred_dp, sensitive_features_test)\n",
    "groups, group_metrics_eo, gaps_eo = evaluate_fairness(y_test, y_pred_eo, sensitive_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[h(X) = 1 | A = 0] = 0.5340909090909091\n",
      "P[h(X) = 1 | A = 1] = 0.5306859205776173\n",
      "Delta_dp = 0.00340498851329174\n",
      "P[h(X) = 1 | A = 0, Y = 1] = 0.9047619047619048\n",
      "P[h(X) = 1 | A = 0, Y = 0] = 0.34328358208955223\n",
      "P[h(X) = 1 | A = 1, Y = 1] = 0.7721518987341772\n",
      "P[h(X) = 1 | A = 1, Y = 0] = 0.20168067226890757\n",
      "Delta_eo1 = 0.13261000602772754\n",
      "Delta_eo0 = 0.14160290982064466\n"
     ]
    }
   ],
   "source": [
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}] = {}\".format(group, group_metrics_dp['dp'][group]))\n",
    "print(\"Delta_dp = {}\".format(gaps_dp['dp']))\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 1] = {}\".format(group, group_metrics_eo['eo_y1'][group]))\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 0] = {}\".format(group, group_metrics_eo['eo_y0'][group]))\n",
    "print(\"Delta_eo1 = {}\".format(gaps_eo['eo_y1']))\n",
    "print(\"Delta_eo0 = {}\".format(gaps_eo['eo_y0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from sklearn import clone\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "class LogisticRegressionAsRegression(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, logistic_regression_estimator):\n",
    "        self.logistic_regression_estimator = logistic_regression_estimator\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        try:\n",
    "            check_is_fitted(self.logistic_regression_estimator)\n",
    "            self.logistic_regression_estimator_ = self.logistic_regression_estimator\n",
    "        except NotFittedError:\n",
    "            self.logistic_regression_estimator_ = clone(\n",
    "                self.logistic_regression_estimator\n",
    "            ).fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # use predict_proba to get real values instead of 0/1, select only prob for 1\n",
    "        scores = self.logistic_regression_estimator_.predict_proba(X)[:, 1]\n",
    "        return scores\n",
    "\n",
    "estimator_wrapper = LogisticRegressionAsRegression(logreg).fit(X_train, y_train)\n",
    "postprocessed_predictor_dp = ThresholdOptimizer(estimator=estimator_wrapper, constraints=\"demographic_parity\", prefit=True)\n",
    "postprocessed_predictor_eo = ThresholdOptimizer(estimator=estimator_wrapper, constraints=\"equalized_odds\", prefit=True)\n",
    "\n",
    "postprocessed_predictor_dp.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "postprocessed_predictor_eo.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "\n",
    "y_pred_dp = postprocessed_predictor_dp.predict(X_test, sensitive_features=sensitive_features_test)\n",
    "y_pred_eo = postprocessed_predictor_eo.predict(X_test, sensitive_features=sensitive_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lawschool Test Accuracy (Postprocessing, DP): 0.7424657534246575\n",
      "Lawschool Test Accuracy (Postprocessing, EO): 0.7095890410958904\n"
     ]
    }
   ],
   "source": [
    "print(\"Lawschool Test Accuracy (Postprocessing, DP): {}\".format(accuracy_score(y_pred_dp, y_test)))\n",
    "print(\"Lawschool Test Accuracy (Postprocessing, EO): {}\".format(accuracy_score(y_pred_eo, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups, group_metrics_dp, gaps_dp = evaluate_fairness(y_test, y_pred_dp, sensitive_features_test)\n",
    "groups, group_metrics_eo, gaps_eo = evaluate_fairness(y_test, y_pred_eo, sensitive_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[h(X) = 1 | A = 0] = 0.6136363636363636\n",
      "P[h(X) = 1 | A = 1] = 0.5379061371841155\n",
      "Delta_dp = 0.07573022645224814\n",
      "P[h(X) = 1 | A = 0, Y = 1] = 0.7619047619047619\n",
      "P[h(X) = 1 | A = 0, Y = 0] = 0.2537313432835821\n",
      "P[h(X) = 1 | A = 1, Y = 1] = 0.6455696202531646\n",
      "P[h(X) = 1 | A = 1, Y = 0] = 0.23529411764705882\n",
      "Delta_eo1 = 0.1163351416515973\n",
      "Delta_eo0 = 0.01843722563652328\n"
     ]
    }
   ],
   "source": [
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}] = {}\".format(group, group_metrics_dp['dp'][group]))\n",
    "print(\"Delta_dp = {}\".format(gaps_dp['dp']))\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 1] = {}\".format(group, group_metrics_eo['eo_y1'][group]))\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 0] = {}\".format(group, group_metrics_eo['eo_y0'][group]))\n",
    "print(\"Delta_eo1 = {}\".format(gaps_eo['eo_y1']))\n",
    "print(\"Delta_eo0 = {}\".format(gaps_eo['eo_y0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/anaconda3/envs/fairness_checking/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/sam/anaconda3/envs/fairness_checking/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/sam/anaconda3/envs/fairness_checking/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/sam/anaconda3/envs/fairness_checking/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv('./../../data/processed/communities/communities_train1_X.csv')\n",
    "X_test = pd.read_csv('./../../data/processed/communities/communities_test1_X.csv')\n",
    "y_train = pd.read_csv('./../../data/processed/communities/communities_train1_y.csv')\n",
    "y_train = y_train['ViolentCrimesPerPop']\n",
    "y_test = pd.read_csv('./../../data/processed/communities/communities_test1_y.csv')\n",
    "y_test = y_test['ViolentCrimesPerPop']\n",
    "\n",
    "sensitive_features_train = X_train['majority_white']\n",
    "sensitive_features_test = X_test['majority_white']\n",
    "sensitive_features_train[sensitive_features_train < 0] = 0\n",
    "sensitive_features_train[sensitive_features_train > 0] = 1\n",
    "sensitive_features_train = sensitive_features_train.reset_index(drop=True)\n",
    "\n",
    "sensitive_features_test[sensitive_features_test < 0] = 0\n",
    "sensitive_features_test[sensitive_features_test > 0] = 1\n",
    "sensitive_features_test = sensitive_features_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/anaconda3/envs/fairness_checking/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communities Test Accuracy (Unfair): 0.8646616541353384\n"
     ]
    }
   ],
   "source": [
    "print(\"Communities Test Accuracy (Unfair): {}\".format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[h(X) = 1 | A = 0] = 0.6379310344827587\n",
      "P[h(X) = 1 | A = 1] = 0.09187279151943463\n",
      "Delta_dp = 0.5460582429633241\n",
      "P[h(X) = 1 | A = 0, Y = 1] = 0.8026315789473685\n",
      "P[h(X) = 1 | A = 0, Y = 0] = 0.325\n",
      "P[h(X) = 1 | A = 1, Y = 1] = 0.5\n",
      "P[h(X) = 1 | A = 1, Y = 0] = 0.024691358024691357\n",
      "Delta_eo1 = 0.3026315789473685\n",
      "Delta_eo0 = 0.30030864197530865\n"
     ]
    }
   ],
   "source": [
    "groups, group_metrics, gaps = evaluate_fairness(y_test, y_pred, sensitive_features_test)\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}] = {}\".format(group, group_metrics['dp'][group]))\n",
    "print(\"Delta_dp = {}\".format(gaps['dp']))\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 1] = {}\".format(group, group_metrics['eo_y1'][group]))\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 0] = {}\".format(group, group_metrics['eo_y0'][group]))\n",
    "print(\"Delta_eo1 = {}\".format(gaps['eo_y1']))\n",
    "print(\"Delta_eo0 = {}\".format(gaps['eo_y0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity, EqualizedOdds\n",
    "\n",
    "expgrad_dp = ExponentiatedGradient(\n",
    "    LogisticRegression(solver='liblinear', fit_intercept=True, class_weight='balanced'),\n",
    "    constraints=DemographicParity(),\n",
    "    eps=0.05,\n",
    "    nu=1e-6)\n",
    "\n",
    "expgrad_eo = ExponentiatedGradient(\n",
    "    LogisticRegression(solver='liblinear', fit_intercept=True, class_weight='balanced'),\n",
    "    constraints=EqualizedOdds(),\n",
    "    eps=0.05,\n",
    "    nu=1e-6)\n",
    "\n",
    "expgrad_dp.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "expgrad_eo.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "y_pred_dp = expgrad_dp.predict(X_test)\n",
    "y_pred_eo = expgrad_eo.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communities Test Accuracy (Inprocessing, DP): 0.7368421052631579\n",
      "Communities Test Accuracy (Inprocessing, EO): 0.8120300751879699\n"
     ]
    }
   ],
   "source": [
    "print(\"Communities Test Accuracy (Inprocessing, DP): {}\".format(accuracy_score(y_pred_dp, y_test)))\n",
    "print(\"Communities Test Accuracy (Inprocessing, EO): {}\".format(accuracy_score(y_pred_eo, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[h(X) = 1 | A = 0] = 0.3879310344827586\n",
      "P[h(X) = 1 | A = 1] = 0.3321554770318021\n",
      "Delta_dp = 0.055775557450956526\n",
      "P[h(X) = 1 | A = 0, Y = 1] = 0.75\n",
      "P[h(X) = 1 | A = 0, Y = 0] = 0.275\n",
      "P[h(X) = 1 | A = 1, Y = 1] = 0.8\n",
      "P[h(X) = 1 | A = 1, Y = 0] = 0.1522633744855967\n",
      "Delta_eo1 = 0.050000000000000044\n",
      "Delta_eo0 = 0.12273662551440331\n"
     ]
    }
   ],
   "source": [
    "groups, group_metrics_dp, gaps_dp = evaluate_fairness(y_test, y_pred_dp, sensitive_features_test)\n",
    "groups, group_metrics_eo, gaps_eo = evaluate_fairness(y_test, y_pred_eo, sensitive_features_test)\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}] = {}\".format(group, group_metrics_dp['dp'][group]))\n",
    "print(\"Delta_dp = {}\".format(gaps_dp['dp']))\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 1] = {}\".format(group, group_metrics_eo['eo_y1'][group]))\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 0] = {}\".format(group, group_metrics_eo['eo_y0'][group]))\n",
    "print(\"Delta_eo1 = {}\".format(gaps_eo['eo_y1']))\n",
    "print(\"Delta_eo0 = {}\".format(gaps_eo['eo_y0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from sklearn import clone\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "class LogisticRegressionAsRegression(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, logistic_regression_estimator):\n",
    "        self.logistic_regression_estimator = logistic_regression_estimator\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        try:\n",
    "            check_is_fitted(self.logistic_regression_estimator)\n",
    "            self.logistic_regression_estimator_ = self.logistic_regression_estimator\n",
    "        except NotFittedError:\n",
    "            self.logistic_regression_estimator_ = clone(\n",
    "                self.logistic_regression_estimator\n",
    "            ).fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # use predict_proba to get real values instead of 0/1, select only prob for 1\n",
    "        scores = self.logistic_regression_estimator_.predict_proba(X)[:, 1]\n",
    "        return scores\n",
    "\n",
    "estimator_wrapper = LogisticRegressionAsRegression(logreg).fit(X_train, y_train)\n",
    "postprocessed_predictor_dp = ThresholdOptimizer(estimator=estimator_wrapper, constraints=\"demographic_parity\", prefit=True)\n",
    "postprocessed_predictor_eo = ThresholdOptimizer(estimator=estimator_wrapper, constraints=\"equalized_odds\", prefit=True)\n",
    "\n",
    "postprocessed_predictor_dp.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "postprocessed_predictor_eo.fit(X_train, y_train, sensitive_features=sensitive_features_train)\n",
    "\n",
    "y_pred_dp = postprocessed_predictor_dp.predict(X_test, sensitive_features=sensitive_features_test)\n",
    "y_pred_eo = postprocessed_predictor_eo.predict(X_test, sensitive_features=sensitive_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communities Test Accuracy (Postprocessing, DP): 0.7619047619047619\n",
      "Communities Test Accuracy (Postprocessing, EO): 0.8270676691729323\n"
     ]
    }
   ],
   "source": [
    "print(\"Communities Test Accuracy (Postprocessing, DP): {}\".format(accuracy_score(y_pred_dp, y_test)))\n",
    "print(\"Communities Test Accuracy (Postprocessing, EO): {}\".format(accuracy_score(y_pred_eo, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P[h(X) = 1 | A = 0] = 0.16379310344827586\n",
      "P[h(X) = 1 | A = 1] = 0.2049469964664311\n",
      "Delta_dp = 0.04115389301815525\n",
      "P[h(X) = 1 | A = 0, Y = 1] = 0.6710526315789473\n",
      "P[h(X) = 1 | A = 0, Y = 0] = 0.225\n",
      "P[h(X) = 1 | A = 1, Y = 1] = 0.775\n",
      "P[h(X) = 1 | A = 1, Y = 0] = 0.10699588477366255\n",
      "Delta_eo1 = 0.10394736842105268\n",
      "Delta_eo0 = 0.11800411522633746\n"
     ]
    }
   ],
   "source": [
    "groups, group_metrics_dp, gaps_dp = evaluate_fairness(y_test, y_pred_dp, sensitive_features_test)\n",
    "groups, group_metrics_eo, gaps_eo = evaluate_fairness(y_test, y_pred_eo, sensitive_features_test)\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}] = {}\".format(group, group_metrics_dp['dp'][group]))\n",
    "print(\"Delta_dp = {}\".format(gaps_dp['dp']))\n",
    "for group in groups:\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 1] = {}\".format(group, group_metrics_eo['eo_y1'][group]))\n",
    "    print(\"P[h(X) = 1 | A = {}, Y = 0] = {}\".format(group, group_metrics_eo['eo_y0'][group]))\n",
    "print(\"Delta_eo1 = {}\".format(gaps_eo['eo_y1']))\n",
    "print(\"Delta_eo0 = {}\".format(gaps_eo['eo_y0']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fairness_checking] *",
   "language": "python",
   "name": "conda-env-fairness_checking-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
