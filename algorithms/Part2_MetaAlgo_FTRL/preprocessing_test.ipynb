{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/anaconda3/envs/fairness_checking/lib/python3.7/site-packages/aif360/algorithms/preprocessing/lfr_helpers/helpers.py:2: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit\n",
      "/Users/sam/anaconda3/envs/fairness_checking/lib/python3.7/site-packages/aif360/algorithms/preprocessing/lfr_helpers/helpers.py:2: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit\n"
     ]
    }
   ],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.preprocessing.reweighing import Reweighing\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fairness(y_true, y_pred, sensitive_features):\n",
    "    \"\"\"\n",
    "    Evaluates fairness of the final majority vote classifier over T_inner hypotheses\n",
    "    on the test set.\n",
    "    #NOTE: defined in the meta_algo file, but we chose:\n",
    "    a0 := African-American (COMPAS), Female (Adult)\n",
    "    a1 := Caucasian (COMPAS), Male (Adult)\n",
    "\n",
    "    :return: list. subgroups in sensitive_features.\n",
    "    :return: list, dict, dict. groups is a list of the sensitive features in the dataset. \n",
    "    group_metrics is a dictionary containing dictionaries that have Delta_dp, Delta_eoy0, \n",
    "    and Delta_eoy1 for each group. gaps is a dictionary that contains the fairness gap\n",
    "    for dp, eo_y0 and eo_y1.\n",
    "    \"\"\"\n",
    "    groups = np.unique(sensitive_features.values)\n",
    "    pos_count = {}\n",
    "    dp_pct = {}\n",
    "    eo_y0_pct = {}\n",
    "    eo_y1_pct = {}\n",
    "\n",
    "    for index, group in enumerate(groups):\n",
    "        # Demographic Parity\n",
    "        indices = {}\n",
    "        indices[group] = sensitive_features.index[sensitive_features == group]\n",
    "        dp_pct[group] = sum(y_pred[indices[group]])/len(indices[group])\n",
    "\n",
    "        # Equalized Odds\n",
    "        y1_indices = {}\n",
    "        y0_indices = {}\n",
    "        y1_indices[group] = sensitive_features.index[(sensitive_features == group) & (y_true == 1)]\n",
    "        y0_indices[group] = sensitive_features.index[(sensitive_features == group) & (y_true == 0)]\n",
    "        eo_y0_pct[group] = sum(y_pred[y0_indices[group]])/len(y0_indices[group])   \n",
    "        eo_y1_pct[group] = sum(y_pred[y1_indices[group]])/len(y1_indices[group])\n",
    "\n",
    "    gaps = {}\n",
    "    group_metrics = {} # a dictionary of dictionaries\n",
    "\n",
    "    gaps['dp'] = abs(dp_pct[groups[0]] - dp_pct[groups[1]])\n",
    "    gaps['eo_y0'] = abs(eo_y0_pct[groups[0]] - eo_y0_pct[groups[1]])\n",
    "    gaps['eo_y1'] = abs(eo_y1_pct[groups[0]] - eo_y1_pct[groups[1]])\n",
    "    group_metrics['dp'] = dp_pct\n",
    "    group_metrics['eo_y0'] = eo_y0_pct\n",
    "    group_metrics['eo_y1'] = eo_y1_pct\n",
    "\n",
    "    return groups, group_metrics, gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_arr = []\n",
    "X_test_arr = []\n",
    "y_train_arr = []\n",
    "y_test_arr = []\n",
    "sensitive_features_train_arr = []\n",
    "sensitive_features_test_arr = []\n",
    "\n",
    "for i in range(5):\n",
    "    X_train = pd.read_csv('./../../data/processed/adult/adult_train{}_X.csv'.format(i + 1))\n",
    "    X_test = pd.read_csv('./../../data/processed/adult/adult_test{}_X.csv'.format(i + 1))\n",
    "    y_train = pd.read_csv('./../../data/processed/adult/adult_train{}_y.csv'.format(i + 1))\n",
    "    y_test = pd.read_csv('./../../data/processed/adult/adult_test{}_y.csv'.format(i + 1))\n",
    "\n",
    "    y_train = y_train['income']\n",
    "    y_test = y_test['income']\n",
    "    \n",
    "    sensitive_features_train = X_train['sex']\n",
    "    sensitive_features_test = X_test['sex']\n",
    "\n",
    "    sensitive_features_train[sensitive_features_train <= 0] = 0\n",
    "    sensitive_features_train[sensitive_features_train > 0] = 1\n",
    "    sensitive_features_train = sensitive_features_train.reset_index(drop=True)\n",
    "    \n",
    "    sensitive_features_test[sensitive_features_test <= 0] = 0\n",
    "    sensitive_features_test[sensitive_features_test > 0] = 1\n",
    "    sensitive_features_test = sensitive_features_test.reset_index(drop=True)\n",
    "    \n",
    "    X_train_arr.append(X_train)\n",
    "    X_test_arr.append(X_test)\n",
    "    y_train_arr.append(y_train)\n",
    "    y_test_arr.append(y_test)\n",
    "    sensitive_features_train_arr.append(sensitive_features_train)\n",
    "    sensitive_features_test_arr.append(sensitive_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_copy = X_train_arr[0].copy()\n",
    "y_train_copy = y_train_arr[0].copy()\n",
    "X_test_copy = X_test_arr[0].copy()\n",
    "y_test_copy = y_test_arr[0].copy()\n",
    "sensitive_features_test = sensitive_features_test_arr[0]\n",
    "\n",
    "X_train_copy['income'] = y_train_copy\n",
    "X_test_copy['income'] = y_test_copy\n",
    "\n",
    "privileged_groups = [{'sex': 1}]\n",
    "unprivileged_groups = [{'sex': 0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset object\n",
    "dataset_orig_train = BinaryLabelDataset(1, 0, df=X_train_copy, label_names=['income'], protected_attribute_names=['sex'])\n",
    "dataset_orig_test = BinaryLabelDataset(1, 0, df=X_test_copy, label_names=['income'], protected_attribute_names=['sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "               privileged_groups=privileged_groups)\n",
    "dataset_transf_train = RW.fit_transform(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing \n",
    "assert np.abs(dataset_transf_train.instance_weights.sum()-dataset_orig_train.instance_weights.sum())<1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Income</th>\n",
       "      <th>Original_weight</th>\n",
       "      <th>new_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.854641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.209897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.854641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.209897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.854641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.854641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.854641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.854641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.854641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.209897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sex  Income  Original_weight  new_weight\n",
       "1589  0.0       1              1.0    0.854641\n",
       "353   0.0       0              1.0    1.209897\n",
       "1028  0.0       1              1.0    0.854641\n",
       "394   0.0       0              1.0    1.209897\n",
       "219   0.0       1              1.0    0.854641\n",
       "751   1.0       0              1.0    0.666474\n",
       "1562  1.0       0              1.0    0.666474\n",
       "1481  1.0       0              1.0    0.666474\n",
       "603   1.0       0              1.0    0.666474\n",
       "1352  1.0       0              1.0    0.666474\n",
       "64    0.0       1              1.0    0.854641\n",
       "1385  0.0       1              1.0    0.854641\n",
       "439   0.0       1              1.0    0.854641\n",
       "884   0.0       1              1.0    0.854641\n",
       "1127  0.0       0              1.0    1.209897"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'Sex': X_train_copy.sex,\n",
    "              'Income': y_train_copy,\n",
    "              'Original_weight': np.ones(shape=(X_train_copy.shape[0],)),\n",
    "              'new_weight': dataset_transf_train.instance_weights}).sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression classifier and predictions\n",
    "lmod = LogisticRegression()\n",
    "lmod.fit(X_train_arr[0], y_train_arr[0], \n",
    "         sample_weight=dataset_transf_train.instance_weights)\n",
    "\n",
    "y_pred = lmod.predict(X_test_arr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8366336633663366"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred, y_test_arr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1.]),\n",
       " {'dp': {0.0: 0.5469798657718121, 1.0: 0.3018867924528302},\n",
       "  'eo_y0': {0.0: 0.1937984496124031, 1.0: 0.1},\n",
       "  'eo_y1': {0.0: 0.8165680473372781, 1.0: 0.9230769230769231}},\n",
       " {'dp': 0.24509307331898195,\n",
       "  'eo_y0': 0.0937984496124031,\n",
       "  'eo_y1': 0.10650887573964507})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_fairness(y_test_arr[0], y_pred, sensitive_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_arr[0], y_train_arr[0])\n",
    "y_pred = logreg.predict(X_test_arr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8316831683168316"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred, y_test_arr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1.]),\n",
       " {'dp': {0.0: 0.587248322147651, 1.0: 0.24528301886792453},\n",
       "  'eo_y0': {0.0: 0.24806201550387597, 1.0: 0.0625},\n",
       "  'eo_y1': {0.0: 0.8461538461538461, 1.0: 0.8076923076923077}},\n",
       " {'dp': 0.34196530327972646,\n",
       "  'eo_y0': 0.18556201550387597,\n",
       "  'eo_y1': 0.038461538461538436})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_fairness(y_test_arr[0], y_pred, sensitive_features_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
