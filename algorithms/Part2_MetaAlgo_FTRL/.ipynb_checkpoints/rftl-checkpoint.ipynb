{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "dataset_used = 'compas'\n",
    "\n",
    "if(dataset_used == 'compas'):\n",
    "    compas_train = pd.read_csv('./../../data/compas_train.csv')\n",
    "    compas_val = pd.read_csv('./../../data/compas_val.csv')\n",
    "    compas_test = pd.read_csv('./../../data/compas_test.csv')\n",
    "\n",
    "    y_train = compas_train.pop('two_year_recid') \n",
    "    y_test = compas_test.pop('two_year_recid')\n",
    "    sensitive_features_train = compas_train['race']\n",
    "    sensitive_features_test = compas_test['race']\n",
    "    X_train = compas_train\n",
    "    X_test = compas_test\n",
    "    X_test_a0 = X_test[X_test.race.eq(0)]\n",
    "    X_test_a1 = X_test[X_test.race.eq(1)]\n",
    "    a_indices = dict()\n",
    "    a_indices['a0'] = X_train.index[X_train.race.eq(0)].tolist()\n",
    "    a_indices['a1'] = X_train.index[X_train.race.eq(1)].tolist()\n",
    "    a_indices['all'] = X_train.race.tolist()\n",
    "    \n",
    "    sensitive_features_train = sensitive_features_train.replace(0, 'African-American')\n",
    "    sensitive_features_train = sensitive_features_train.replace(1, 'Caucasian')\n",
    "    sensitive_features_test = sensitive_features_test.replace(0, 'African-American')\n",
    "    sensitive_features_test = sensitive_features_test.replace(1, 'Caucasian')\n",
    "    \n",
    "elif(dataset_used == 'adult'):\n",
    "    adult_train = pd.read_csv('./../../data/adult_train.csv')\n",
    "    adult_val = pd.read_csv('./../../data/adult_val.csv')\n",
    "    adult_test = pd.read_csv('./../../data/adult_test.csv')\n",
    "\n",
    "    y_train = adult_train.pop('Income Binary') \n",
    "    y_test = adult_test.pop('Income Binary')\n",
    "    sensitive_features_train = adult_train['sex']\n",
    "    sensitive_features_test = adult_test['sex']\n",
    "    X_train = adult_train\n",
    "    X_test = adult_test\n",
    "    X_test_a0 = X_test[X_test.sex.eq(0)]\n",
    "    X_test_a1 = X_test[X_test.sex.eq(1)]\n",
    "    a_indices = dict()\n",
    "    a_indices['a0'] = X_train.index[X_train.sex.eq(0)].tolist()\n",
    "    a_indices['a1'] = X_train.index[X_train.sex.eq(1)].tolist()\n",
    "    a_indices['all'] = X_train.sex.tolist()\n",
    "    \n",
    "    sensitive_features_train = sensitive_features_train.replace(0, 'Female')\n",
    "    sensitive_features_train = sensitive_features_train.replace(1, 'Male')\n",
    "    sensitive_features_test = sensitive_features_test.replace(0, 'Female')\n",
    "    sensitive_features_test = sensitive_features_test.replace(1, 'Male')\n",
    "    \n",
    "else:\n",
    "    print('Invalid dataset_used variable.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hyperparameters (M, B, T, gamma_1, gamma_2)\n",
    "card_A = 2 # Cardinality of A\n",
    "nu = 0.01 # 0.001 is too large for efficiency\n",
    "M = 1\n",
    "B = M\n",
    "T = len(X_test)/(nu ** 2)\n",
    "gamma_1 = nu/B\n",
    "gamma_2 = nu/B\n",
    "delta_1 = (2 * len(X_train)) / gamma_1\n",
    "delta_2 = (2 * card_A) / gamma_2\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00023685 0.00023685 0.00023685 ... 0.00023685 0.00023685 0.00023685]\n",
      "4222\n"
     ]
    }
   ],
   "source": [
    "### Set mock t_weights that should come from Meta-Algo ###\n",
    "t_weights = np.full((X_train.shape[0],), 1/X_train.shape[0])\n",
    "print(t_weights)\n",
    "print(len(t_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_w(w, buckets):\n",
    "    for i, w_i in enumerate(w):\n",
    "        for b in buckets:\n",
    "            if(b[0] <= w_i <= b[1]):\n",
    "                w[i] = b[1]\n",
    "    \n",
    "    return w\n",
    "\n",
    "def generate_w():\n",
    "    r = [ran.random() for i in range(len(X_train))]\n",
    "    s = sum(r)\n",
    "    r = [ i/s for i in r ]\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize N(gamma_1, W)\n",
    "gamma_1_num_buckets = np.ceil(math.log(delta_1, 1 + gamma_1))\n",
    "gamma_1_buckets = []\n",
    "gamma_1_buckets.append((0, 1/delta_1))\n",
    "for i in range(int(gamma_1_num_buckets)):\n",
    "    bucket_lower = ((1 + gamma_1) ** i) * (1/delta_1)\n",
    "    bucket_upper = ((1 + gamma_1) ** (i + 1)) * (1/delta_1)\n",
    "    gamma_1_buckets.append((bucket_lower, bucket_upper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity, EqualizedOdds\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def best_response_lambda(h):      \n",
    "    lambda_w_a_ap = B\n",
    "    if(ran.randint(0, 1) == 0):\n",
    "        return ('a0', 'a1', tuple(discretize_w(generate_w(), gamma_1_buckets)))\n",
    "    else:\n",
    "        return ('a1', 'a0', tuple(discretize_w(generate_w(), gamma_1_buckets)))\n",
    "    \n",
    "def test_lambda_dict():\n",
    "    lambda_dict = defaultdict(int)\n",
    "    \n",
    "    for i in range(50):\n",
    "        lambda_dict[('a0', 'a1', tuple(discretize_w(generate_w(), gamma_1_buckets)))] = 1\n",
    "    \n",
    "    for i in range(50):\n",
    "        lambda_dict[('a1', 'a0', tuple(discretize_w(generate_w(), gamma_1_buckets)))] = 1\n",
    "        \n",
    "    return lambda_dict\n",
    "\n",
    "lambda_dict = test_lambda_dict()\n",
    "\n",
    "# L_i is just c^1_i(\\lambda) - c^0_i(\\lambda)\n",
    "def L_i(i, y_i, t_weights, lambda_dict, a_indices):\n",
    "    return c_1_i(i, y_i, t_weights, lambda_dict, a_indices) - c_0_i(i, y_i, t_weights, lambda_dict)\n",
    "\n",
    "def zero_one_loss(y_pred, y_true):\n",
    "    if(y_pred == y_true):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def c_1_i(i, y_i, t_weights, lambda_dict, a_indices):\n",
    "    return zero_one_loss(1, y_i)*t_weights[i] + delta_i(i, t_weights, lambda_dict, a_indices)\n",
    "    \n",
    "def c_0_i(i, y_i, t_weights, lambda_dict):\n",
    "    return zero_one_loss(0, y_i)*t_weights[i]\n",
    "\n",
    "def delta_i(i, t_weights, lambda_dict, a_indices):\n",
    "    # get a_i\n",
    "    if(a_indices['all'][i] == 0):\n",
    "        a_i = 'a0'\n",
    "        a_p = 'a1'\n",
    "    else:\n",
    "        a_i = 'a1'\n",
    "        a_p = 'a0'\n",
    "        \n",
    "    # weights quotient\n",
    "    quotient = t_weights[i]/t_weights[a_indices[a_i]].sum()  \n",
    "    \n",
    "    # lambda difference. iterate over all keys of lambda_dict (the rest are 0)\n",
    "    final_sum = 0\n",
    "    for tup in lambda_dict:\n",
    "        if(tup[0] == a_i):\n",
    "            if (tup[1], tup[0], tup[2]) in lambda_dict:\n",
    "                diff = lambda_dict[tup] - lambda_dict[(tup[1], tup[0], tup[2])]\n",
    "            else:\n",
    "                diff = lambda_dict[tup]\n",
    "                \n",
    "        elif(tup[0] == a_p):\n",
    "            if (tup[1], tup[0], tup[2]) in lambda_dict:\n",
    "                diff = lambda_dict[(tup[1], tup[0], tup[2])] - lambda_dict[tup] \n",
    "            else:\n",
    "                diff = - lambda_dict[tup]\n",
    "\n",
    "        final_sum += diff*quotient\n",
    "    \n",
    "    return final_sum\n",
    "    \n",
    "def weighted_classification(X_train, y_train, t_weights, sensitive_features_train, a_indices, \n",
    "                            lambda_dict, constraint_used='dp', eta=1/np.sqrt(2*T)):\n",
    "    \n",
    "    # Learning becomes a weighted classification problem, dependent on L_i\n",
    "    weights = []\n",
    "    for i in range(len(X_train)):\n",
    "        y_i = y_train[i]\n",
    "        weights.append(eta * L_i(i, y_i, t_weights, lambda_dict, a_indices) + 0.5)\n",
    "        \n",
    "    if(constraint_used =='dp'):\n",
    "        expgrad_X = ExponentiatedGradient(\n",
    "            LogisticRegression(solver='liblinear', fit_intercept=True, class_weight='balanced'),\n",
    "            constraints=DemographicParity(),\n",
    "            eps=0.01,\n",
    "            nu=1e-6)\n",
    "    elif(constraint_used == 'eo'):\n",
    "        expgrad_X = ExponentiatedGradient(\n",
    "            LogisticRegression(solver='liblinear', fit_intercept=True, class_weight='balanced'),\n",
    "            constraints=EqualizedOdds(),\n",
    "            eps=0.01,\n",
    "            nu=1e-6)\n",
    "        \n",
    "    expgrad_X.fit(X_train, y_train, sensitive_features=sensitive_features_train, sample_weight=weights)\n",
    "    return expgrad_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN ALGORITHM ###\n",
    "# On time step t...\n",
    "\n",
    "hypotheses = []\n",
    "h_1 = 0\n",
    "hypotheses.append(h_1)\n",
    "for t in range(int(T)):\n",
    "    lambda_t = best_response_lambda(h)\n",
    "    lambda_dict[lambda_t] += B\n",
    "    h_t = weighted_classification(X_train, y_train, t_weights, \n",
    "                                        sensitive_features_train, a_indices, lambda_dict)\n",
    "    hypotheses.append(h_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
