{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sklearn.datasets.load_iris()\n",
    "X = iris.data[:, :2]\n",
    "y = (iris.target != 0)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, iters=100000, verbose=False):\n",
    "        self.lr = lr\n",
    "        self.iters = iters\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "      return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "    def predict(self, X, theta):\n",
    "        z = np.dot(X, theta)\n",
    "        return self.sigmoid(z)\n",
    "\n",
    "    def loss_function(self, X, y, theta):\n",
    "        n = len(y)\n",
    "        pred = self.predict(X, theta)\n",
    "\n",
    "        class1_cost = -y * np.log(pred)\n",
    "        class0_cost = -(1 - y) * np.log(1 - pred)\n",
    "        loss = class1_cost + class0_cost\n",
    "\n",
    "        loss = loss.sum() / n\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def update_theta(self, X, y, theta):\n",
    "        n = len(y)\n",
    "        pred = self.predict(X, theta)\n",
    "\n",
    "        gradient_theta = np.dot(X.T, pred - y)\n",
    "\n",
    "        gradient_theta /= n\n",
    "        theta -= self.lr * gradient_theta\n",
    "        return theta\n",
    "\n",
    "    def classify(self, X, theta):\n",
    "      return (self.predict(X,theta) >= 0.5).astype(int)\n",
    "\n",
    "    def grad_descent_theta(self, X, y, theta):\n",
    "        cost_history = []\n",
    "\n",
    "        for i in range(self.iters):\n",
    "            weights = self.update_theta(X, y, theta)\n",
    "\n",
    "            #Calculate error for auditing purposes\n",
    "            cost = self.loss_function(X, y, theta)\n",
    "            cost_history.append(cost)\n",
    "\n",
    "            # Log Progress\n",
    "            if i % 1000 == 0:\n",
    "                print(\"iter: \"+str(i) + \" cost: \"+str(cost))\n",
    "\n",
    "        return weights, cost_history\n",
    "\n",
    "    def accuracy(predicted_labels, actual_labels):\n",
    "        diff = predicted_labels - actual_labels\n",
    "        return 1.0 - (float(np.count_nonzero(diff)) / len(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0 cost: 0.6687074232261783\n",
      "iter: 1000 cost: 0.3184330521849697\n",
      "iter: 2000 cost: 0.2204488607083133\n",
      "iter: 3000 cost: 0.1737708777886455\n",
      "iter: 4000 cost: 0.1464501661616072\n",
      "iter: 5000 cost: 0.12843289951920084\n",
      "iter: 6000 cost: 0.11560521808395043\n",
      "iter: 7000 cost: 0.10597489063770808\n",
      "iter: 8000 cost: 0.09845894959612554\n",
      "iter: 9000 cost: 0.09241714824336872\n",
      "iter: 10000 cost: 0.08744608183886389\n",
      "iter: 11000 cost: 0.08327855093751765\n",
      "iter: 12000 cost: 0.07973030578848482\n",
      "iter: 13000 cost: 0.07667001960143219\n",
      "iter: 14000 cost: 0.07400145797125485\n",
      "iter: 15000 cost: 0.07165242326210398\n",
      "iter: 16000 cost: 0.06956764447067916\n",
      "iter: 17000 cost: 0.06770405890644514\n",
      "iter: 18000 cost: 0.06602759481738955\n",
      "iter: 19000 cost: 0.0645109246626808\n",
      "iter: 20000 cost: 0.06313186289030547\n",
      "iter: 21000 cost: 0.0618722017858741\n",
      "iter: 22000 cost: 0.06071685135122951\n",
      "iter: 23000 cost: 0.059653194169348445\n",
      "iter: 24000 cost: 0.05867059487844514\n",
      "iter: 25000 cost: 0.05776002254967247\n",
      "iter: 26000 cost: 0.05691375667076836\n",
      "iter: 27000 cost: 0.05612515583558932\n",
      "iter: 28000 cost: 0.05538847401852661\n",
      "iter: 29000 cost: 0.054698713351245466\n",
      "iter: 30000 cost: 0.05405150518135507\n",
      "iter: 31000 cost: 0.05344301324774141\n",
      "iter: 32000 cost: 0.05286985430086913\n",
      "iter: 33000 cost: 0.05232903259407781\n",
      "iter: 34000 cost: 0.0518178854871644\n",
      "iter: 35000 cost: 0.051334038014979766\n",
      "iter: 36000 cost: 0.05087536473655277\n",
      "iter: 37000 cost: 0.05043995753353999\n",
      "iter: 38000 cost: 0.05002609829869662\n",
      "iter: 39000 cost: 0.049632235665892206\n",
      "iter: 40000 cost: 0.049256965097878094\n",
      "iter: 41000 cost: 0.04889901177749839\n",
      "iter: 42000 cost: 0.04855721585052066\n",
      "iter: 43000 cost: 0.048230519649864134\n",
      "iter: 44000 cost: 0.047917956596349155\n",
      "iter: 45000 cost: 0.04761864152371701\n",
      "iter: 46000 cost: 0.047331762218265565\n",
      "iter: 47000 cost: 0.04705657199809988\n",
      "iter: 48000 cost: 0.04679238318532037\n",
      "iter: 49000 cost: 0.04653856134772993\n",
      "iter: 50000 cost: 0.046294520205819305\n",
      "iter: 51000 cost: 0.0460597171166742\n",
      "iter: 52000 cost: 0.045833649059650226\n",
      "iter: 53000 cost: 0.045615849059682106\n",
      "iter: 54000 cost: 0.045405882993322495\n",
      "iter: 55000 cost: 0.04520334673036795\n",
      "iter: 56000 cost: 0.04500786357046757\n",
      "iter: 57000 cost: 0.04481908193965147\n",
      "iter: 58000 cost: 0.0446366733164117\n",
      "iter: 59000 cost: 0.04446033036097338\n",
      "iter: 60000 cost: 0.044289765224804364\n",
      "iter: 61000 cost: 0.04412470802034082\n",
      "iter: 62000 cost: 0.04396490543341544\n",
      "iter: 63000 cost: 0.04381011946303576\n",
      "iter: 64000 cost: 0.04366012627502592\n",
      "iter: 65000 cost: 0.043514715157657986\n",
      "iter: 66000 cost: 0.04337368756879796\n",
      "iter: 67000 cost: 0.04323685626530739\n",
      "iter: 68000 cost: 0.04310404450650091\n",
      "iter: 69000 cost: 0.04297508532438188\n",
      "iter: 70000 cost: 0.04284982085418939\n",
      "iter: 71000 cost: 0.04272810171949837\n",
      "iter: 72000 cost: 0.042609786466732386\n",
      "iter: 73000 cost: 0.042494741044502564\n",
      "iter: 74000 cost: 0.04238283832366239\n",
      "iter: 75000 cost: 0.04227395765439837\n",
      "iter: 76000 cost: 0.04216798445705197\n",
      "iter: 77000 cost: 0.04206480984370016\n",
      "iter: 78000 cost: 0.04196433026782098\n",
      "iter: 79000 cost: 0.041866447199634026\n",
      "iter: 80000 cost: 0.04177106682493613\n",
      "iter: 81000 cost: 0.041678099765467436\n",
      "iter: 82000 cost: 0.041587460819024515\n",
      "iter: 83000 cost: 0.04149906871770781\n",
      "iter: 84000 cost: 0.04141284590283829\n",
      "iter: 85000 cost: 0.04132871831521254\n",
      "iter: 86000 cost: 0.041246615199486324\n",
      "iter: 87000 cost: 0.041166468921584584\n",
      "iter: 88000 cost: 0.041088214798134105\n",
      "iter: 89000 cost: 0.041011790937000116\n",
      "iter: 90000 cost: 0.04093713808809234\n",
      "iter: 91000 cost: 0.04086419950367194\n",
      "iter: 92000 cost: 0.040792920807459956\n",
      "iter: 93000 cost: 0.040723249871904066\n",
      "iter: 94000 cost: 0.04065513670301566\n",
      "iter: 95000 cost: 0.04058853333223407\n",
      "iter: 96000 cost: 0.040523393714823976\n",
      "iter: 97000 cost: 0.04045967363434562\n",
      "iter: 98000 cost: 0.04039733061277951\n",
      "iter: 99000 cost: 0.0403363238259164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4.39051748e-03, 1.52921763e-01, 8.55537000e-03, 1.28230210e-02,\n",
       "       9.63208812e-04, 4.92471824e-04, 7.18479045e-04, 6.59444532e-03,\n",
       "       2.85921585e-02, 6.43713925e-02, 3.38093819e-03, 2.17992327e-03,\n",
       "       9.38461291e-02, 6.39398815e-03, 1.73066148e-03, 2.09796616e-05,\n",
       "       4.92471824e-04, 4.39051748e-03, 6.80114398e-03, 2.44034712e-04,\n",
       "       5.77489078e-02, 6.40082406e-04, 1.04416271e-04, 2.94679082e-02,\n",
       "       2.17992327e-03, 2.39361977e-01, 6.59444532e-03, 7.62838108e-03,\n",
       "       1.97715943e-02, 8.55537000e-03, 3.79705106e-02, 5.77489078e-02,\n",
       "       2.35510161e-05, 4.75375668e-05, 6.43713925e-02, 4.37074224e-02,\n",
       "       3.91219416e-02, 5.52799271e-04, 1.10928842e-02, 1.14389599e-02,\n",
       "       2.52346922e-03, 9.43649736e-01, 1.62655695e-03, 2.52346922e-03,\n",
       "       2.44034712e-04, 9.38461291e-02, 2.44034712e-04, 4.92599937e-03,\n",
       "       1.94237151e-03, 1.71201855e-02, 9.99673963e-01, 9.90933471e-01,\n",
       "       9.99783385e-01, 9.99769500e-01, 9.99889289e-01, 9.90650016e-01,\n",
       "       9.59832063e-01, 9.83311883e-01, 9.99833355e-01, 9.45279352e-01,\n",
       "       9.99794662e-01, 9.79061262e-01, 9.99994540e-01, 9.97324788e-01,\n",
       "       9.58616863e-01, 9.99342099e-01, 8.98250522e-01, 9.97940757e-01,\n",
       "       9.99998203e-01, 9.99090176e-01, 8.71650099e-01, 9.98978775e-01,\n",
       "       9.99981378e-01, 9.98978775e-01, 9.99493817e-01, 9.99562849e-01,\n",
       "       9.99979096e-01, 9.99749169e-01, 9.95345994e-01, 9.98631065e-01,\n",
       "       9.99395404e-01, 9.99395404e-01, 9.97940757e-01, 9.99321351e-01,\n",
       "       7.43942808e-01, 6.32265593e-01, 9.99342099e-01, 9.99997295e-01,\n",
       "       8.98250522e-01, 9.98415124e-01, 9.95852032e-01, 9.93010729e-01,\n",
       "       9.99214212e-01, 9.96303252e-01, 9.93769095e-01, 9.38981650e-01,\n",
       "       9.75833014e-01, 9.98463536e-01, 9.85555884e-01, 9.90650016e-01,\n",
       "       9.59832063e-01, 9.97940757e-01, 9.99972826e-01, 9.99117986e-01,\n",
       "       9.99238234e-01, 9.99998311e-01, 9.57366532e-01, 9.99996592e-01,\n",
       "       9.99997983e-01, 9.94937478e-01, 9.94778544e-01, 9.99926450e-01,\n",
       "       9.99856088e-01, 9.99477851e-01, 9.94614648e-01, 9.90933471e-01,\n",
       "       9.99238234e-01, 9.97827886e-01, 9.99999980e-01, 9.99994540e-01,\n",
       "       9.99431811e-01, 9.83814167e-01, 9.99999859e-01, 9.99871799e-01,\n",
       "       9.95487733e-01, 9.99892675e-01, 9.99413890e-01, 9.93010729e-01,\n",
       "       9.99807031e-01, 9.99984411e-01, 9.99999255e-01, 9.99284098e-01,\n",
       "       9.99807031e-01, 9.99663677e-01, 9.99851547e-01, 9.99999031e-01,\n",
       "       9.01055393e-01, 9.96525211e-01, 9.87879701e-01, 9.99783385e-01,\n",
       "       9.99342099e-01, 9.99783385e-01, 9.97940757e-01, 9.99009987e-01,\n",
       "       9.95487733e-01, 9.99749169e-01, 9.99981378e-01, 9.99238234e-01,\n",
       "       8.39339330e-01, 9.79061262e-01])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "theta = np.full((X.shape[1],), 1/X.shape[0])\n",
    "logreg.grad_descent_theta(X, y, theta)\n",
    "logreg.predict(X, theta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
